     "image/png": "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"
    },
    "f717c664-605d-48d7-b534-deec99087214.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to force tool-calling agent to structure output\n",
    "\n",
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Prerequisites</p>\n",
    "    <p>\n",
    "        This guide assumes familiarity with the following:\n",
    "        <ul>\n",
    "            <li>\n",
    "                <a href=\"https://python.langchain.com/docs/concepts/#structured-output\">\n",
    "                    Structured Output\n",
    "                </a>\n",
    "            </li>            \n",
    "            <li>\n",
    "                <a href=\"https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#tool-calling-agent\">\n",
    "                    Tool calling agent\n",
    "                </a>\n",
    "            </li>                \n",
    "            <li>\n",
    "                <a href=\"https://python.langchain.com/docs/concepts/#chat-models\">\n",
    "                    Chat Models\n",
    "                </a>\n",
    "            </li>\n",
    "            <li>\n",
    "                <a href=\"https://python.langchain.com/docs/concepts/#messages\">\n",
    "                    Messages\n",
    "                </a>\n",
    "            </li>\n",
    "            <li>\n",
    "                <a href=\"https://langchain-ai.github.io/langgraph/concepts/low_level/\">\n",
    "                    LangGraph Glossary\n",
    "                </a>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </p>\n",
    "</div> \n",
    "\n",
    "You might want your agent to return its output in a structured format. For example, if the output of the agent is used by some other downstream software, you may want the output to be in the same structured format every time the agent is invoked to ensure consistency.\n",
    "\n",
    "This notebook will walk through two different options for forcing a tool calling agent to structure its output. We will be using a basic [ReAct agent](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/) (a model node and a tool-calling node) together with a third node at the end that will format response for the user. Both of the options will use the same graph structure as shown in the diagram below, but will have different mechanisms under the hood.\n",
    "\n",
    "![react_diagrams.png](attachment:59e8ed35-f2b4-421e-8d21-880e7ab31e5f.png)\n",
    "\n",
    "**Option 1**\n",
    "\n",
    "![option1.png](attachment:f717c664-605d-48d7-b534-deec99087214.png)\n",
    "\n",
    "The first way you can force your tool calling agent to have structured output is to bind the output you would like as an additional tool for the `agent` node to use. In contrast to the basic ReAct agent, the `agent` node in this case is not selecting between `tools` and `END` but rather selecting between the specific tools it calls. The expected flow in this case is that the LLM in the `agent` node will first select the action tool, and after receiving the action tool output it will call the response tool, which will then route to the `respond` node which simply structures the arguments from the `agent` node tool call.\n",
    "\n",
    "**Pros and Cons**\n",
    "\n",
    "The benefit to this format is that you only need one LLM, and can save money and latency because of this. The downside to this option is that it isn't guaranteed that the single LLM will call the correct tool when you want it to. We can help the LLM by setting `tool_choice` to `any` when we use `bind_tools` which forces the LLM to select at least one tool at every turn, but this is far from a foolproof strategy. In addition, another downside is that the agent might call *multiple* tools, so we need to check for this explicitly in our routing function (or if we are using OpenAI we can set `parallell_tool_calling=False` to ensure only one tool is called at a time).\n",
    "\n",
    "**Option 2**\n",
    "\n",
    "![option2.png](attachment:e9ef3df1-dbc0-4ff0-8040-0280372d67ac.png)\n",
    "\n",
    "The second way you can force your tool calling agent to have structured output is to use a second LLM (in this case `model_with_structured_output`) to respond to the user. \n",
    "\n",
    "In this case, you will define a basic ReAct agent normally, but instead of having the `agent` node choose between the `tools` node and ending the conversation, the `agent` node will choose between the `tools` node and the `respond` node. The `respond` node will contain a second LLM that uses structured output, and once called will return directly to the user. You can think of this method as basic ReAct with one extra step before responding to the user. \n",
    "\n",
    "**Pros and Cons**\n",
    "\n",
    "The benefit of this method is that it guarantees structured output (as long as `.with_structured_output` works as expected with the LLM). The downside to using this approach is that it requires making an additional LLM call before responding to the user, which can increase costs as well as latency. In addition, by not providing the `agent` node LLM with information about the desired output schema there is a risk that the `agent` LLM will fail to call the correct tools required to answer in the correct output schema.\n",
    "\n",
    "Note that both of these options will follow the exact same graph structure (see the diagram above), in that they are both exact replicas of the basic ReAct architecture but with a `respond` node before the end.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's install the required packages and set our API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langgraph langchain_anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n",
    "    <p style=\"padding-top: 5px;\">\n",
    "        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph  read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model, tools, and graph state\n",
    "\n",
    "Now we can define how we want to structure our output, define our graph state, and also our tools and the models we are going to use.\n",
    "\n",
    "To use structured output, we will use the `with_structured_output` method from LangChain, which you can read more about [here](https://python.langchain.com/docs/how_to/structured_output/).\n",
    "\n",
    "We are going to use a single tool in this example for finding the weather, and will return a structured weather response to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "from langchain_core.tools import tool\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "\n",
    "class WeatherResponse(BaseModel):\n",
    "    \"\"\"Respond to the user with this\"\"\"\n",
    "\n",
    "    temperature: float = Field(description=\"The temperature in fahrenheit\")\n",
    "    wind_directon: str = Field(\n",
    "        description=\"The direction of the wind in abbreviated form\"\n",
    "    )\n",
    "    wind_speed: float = Field(description=\"The speed of the wind in km/h\")\n",
    "\n",
    "\n",
    "# Inherit 'messages' key from MessagesState, which is a list of chat messages\n",
    "class AgentState(MessagesState):\n",
    "    # Final structured response from the agent\n",
    "    final_response: WeatherResponse\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(city: Literal[\"nyc\", \"sf\"]):\n",
    "    \"\"\"Use this to get weather information.\"\"\"\n",
    "    if city == \"nyc\":\n",
    "        return \"It is cloudy in NYC, with 5 mph winds in the North-East direction and a temperature of 70 degrees\"\n",
    "    elif city == \"sf\":\n",
    "        return \"It is 75 degrees and sunny in SF, with 3 mph winds in the South-East direction\"\n",
    "    else:\n",
    "        raise AssertionError(\"Unknown city\")\n",
    "\n",
    "\n",
    "tools = [get_weather]\n",
    "\n",
    "model = ChatAnthropic(model=\"claude-3-opus-20240229\")\n",
    "\n",
    "model_with_tools = model.bind_tools(tools)\n",
    "model_with_structured_output = model.with_structured_output(WeatherResponse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Bind output as tool\n",
    "\n",
    "Let's now examine how we would use the single LLM option.\n",
    "\n",
    "### Define Graph\n",
    "\n",
    "The graph definition is very similar to the one above, the only difference is we no longer call an LLM in the `response` node, and instead bind the `WeatherResponse` tool to our LLM that already contains the `get_weather` tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "tools = [get_weather, WeatherResponse]\n",
    "\n",
    "# Force the model to use tools by passing tool_choice=\"any\"\n",
    "model_with_response_tool = model.bind_tools(tools, tool_choice=\"any\")\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: AgentState):\n",
    "    response = model_with_response_tool.invoke(state[\"messages\"])\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Define the function that responds to the user\n",
    "def respond(state: AgentState):\n",
    "    # Construct the final answer from the arguments of the last tool call\n",
    "    weather_tool_call = state[\"messages\"][-1].tool_calls[0]\n",
    "    response = WeatherResponse(**weather_tool_call[\"args\"])\n",
    "    # Since we're using tool calling to return structured output,\n",
    "    # we need to add  a tool message corresponding to the WeatherResponse tool call,\n",
    "    # This is due to LLM providers' requirement that AI messages with tool calls\n",
    "    # need to be followed by a tool message for each tool call\n",
    "    tool_message = {\n",
    "        \"type\": \"tool\",\n",
    "        \"content\": \"Here is your structured response\",\n",
    "        \"tool_call_id\": weather_tool_call[\"id\"],\n",
    "    }\n",
    "    # We return the final answer\n",
    "    return {\"final_response\": response, \"messages\": [tool_message]}\n",
    "\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state: AgentState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If there is only one tool call and it is the response tool call we respond to the user\n",
    "    if (\n",
    "        len(last_message.tool_calls) == 1\n",
    "        and last_message.tool_calls[0][\"name\"] == \"WeatherResponse\"\n",
    "    ):\n",
    "        return \"respond\"\n",
    "    # Otherwise we will use the tool node again\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"respond\", respond)\n",
    "workflow.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"tools\",\n",
    "        \"respond\": \"respond\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "workflow.add_edge(\"respond\", END)\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "\n",
    "Now we can run our graph to check that it worked as intended:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = graph.invoke(input={\"messages\": [(\"human\", \"what's the weather in SF?\")]})[\n",
    "    \"final_response\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WeatherResponse(temperature=75.0, wind_directon='SE', wind_speed=3.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the agent returned a `WeatherResponse` object as we expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: 2 LLMs\n",
    "\n",
    "Let's now dive into how we would use a second LLM to force structured output.\n",
    "\n",
    "### Define Graph\n",
    "\n",
    "We can now define our graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: AgentState):\n",
    "    response = model_with_tools.invoke(state[\"messages\"])\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Define the function that responds to the user\n",
    "def respond(state: AgentState):\n",
    "    # We call the model with structured output in order to return the same format to the user every time\n",
    "    # state['messages'][-2] is the last ToolMessage in the convo, which we convert to a HumanMessage for the model to use\n",
    "    # We could also pass the entire chat history, but this saves tokens since all we care to structure is the output of the tool\n",
    "    response = model_with_structured_output.invoke(\n",
    "        [HumanMessage(content=state[\"messages\"][-2].content)]\n",
    "    )\n",
    "    # We return the final answer\n",
    "    return {\"final_response\": response}\n",
    "\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state: AgentState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If there is no function call, then we respond to the user\n",
    "    if not last_message.tool_calls:\n",
    "        return \"respond\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"respond\", respond)\n",
    "workflow.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"tools\",\n",
    "        \"respond\": \"respond\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "workflow.add_edge(\"respond\", END)\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Usage\n",
    "\n",
    "We can now invoke our graph to verify that the output is being structured as desired:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = graph.invoke(input={\"messages\": [(\"human\", \"what's the weather in SF?\")]})[\n",
    "    \"final_response\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WeatherResponse(temperature=75.0, wind_directon='SE', wind_speed=4.83)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": 
```
> [truncated]

---
https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/react_diagrams.png
```
Content skipped (binary or ignored type).
```
---
https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/run-id-langsmith.md
```md
# How to pass custom run ID or set tags and metadata for graph runs in LangSmith

!!! tip "Prerequisites"
    This guide assumes familiarity with the following:
    
    - [LangSmith Documentation](https://docs.smith.langchain.com)
    - [LangSmith Platform](https://smith.langchain.com)
    - [RunnableConfig](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig)
    - [Add metadata and tags to traces](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces)
    - [Customize run name](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#customize-run-name)

Debugging graph runs can sometimes be difficult to do in an IDE or terminal. [LangSmith](https://docs.smith.langchain.com) lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph  read the [LangSmith documentation](https://docs.smith.langchain.com) for more information on how to get started.

To make it easier to identify and analyzed traces generated during graph invocation, you can set additional configuration at run time (see [RunnableConfig](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig)):

| **Field**   | **Type**            | **Description**                                                                                                    |
|-------------|---------------------|--------------------------------------------------------------------------------------------------------------------|
| run_name    | `str`               | Name for the tracer run for this call. Defaults to the name of the class.                                          |
| run_id      | `UUID`              | Unique identifier for the tracer run for this call. If not provided, a new UUID will be generated.                 |
| tags        | `List[str]`         | Tags for this call and any sub-calls (e.g., a Chain calling an LLM). You can use these to filter calls.            |
| metadata    | `Dict[str, Any]`    | Metadata for this call and any sub-calls (e.g., a Chain calling an LLM). Keys should be strings, values should be JSON-serializable. |

LangGraph graphs implement the [LangChain Runnable Interface](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html) and accept a second argument (`RunnableConfig`) in methods like `invoke`, `ainvoke`, `stream` etc.

The LangSmith platform will allow you to search and filter traces based on `run_name`, `run_id`, `tags` and `metadata`.

## TLDR

```python
import uuid
# Generate a random UUID -- it must be a UUID
config = {"run_id": uuid.uuid4()}, "tags": ["my_tag1"], "metadata": {"a": 5}}
# Works with all standard Runnable methods 
# like invoke, batch, ainvoke, astream_events etc
graph.stream(inputs, config, stream_mode="values")
```

The rest of the how to guide will show a full agent.

## Setup

First, let's install the required packages and set our API keys

```python
%%capture --no-stderr
%pip install --quiet -U langgraph langchain_openai
```

```python
import getpass
import os


def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")


_set_env("OPENAI_API_KEY")
_set_env("LANGSMITH_API_KEY")
```

!!! tip
    Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. [LangSmith](https://docs.smith.langchain.com) lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph  read more about how to get started [here](https://docs.smith.langchain.com).

## Define the graph

For this example we will use the [prebuilt ReAct agent](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/).

```python
from langchain_openai import ChatOpenAI
from typing import Literal
from langgraph.prebuilt import create_react_agent
from langchain_core.tools import tool

# First we initialize the model we want to use.
model = ChatOpenAI(model="gpt-4o", temperature=0)


# For this tutorial we will use custom tool that returns pre-defined values for weather in two cities (NYC & SF)
@tool
def get_weather(city: Literal["nyc", "sf"]):
    """Use this to get weather information."""
    if city == "nyc":
        return "It might be cloudy in nyc"
    elif city == "sf":
        return "It's always sunny in sf"
    else:
        raise AssertionError("Unknown city")


tools = [get_weather]


# Define the graph
graph = create_react_agent(model, tools=tools)
```

## Run your graph

Now that we've defined our graph let's run it once and view the trace in LangSmith. In order for our trace to be easily accessible in LangSmith, we will pass in a custom `run_id` in the config.

This assumes that you have set your `LANGSMITH_API_KEY` environment variable.

Note that you can also configure what project to trace to by setting the `LANGCHAIN_PROJECT` environment variable, by default runs will be traced to the `default` project.

```python
import uuid


def print_stream(stream):
    for s in stream:
        message = s["messages"][-1]
        if isinstance(message, tuple):
            print(message)
        else:
            message.pretty_print()


inputs = {"messages": [("user", "what is the weather in sf")]}

config = {"run_name": "agent_007", "tags": ["cats are awesome"]}

print_stream(graph.stream(inputs, config, stream_mode="values"))
```

**Output:**
```
================================ Human Message ==================================

what is the weather in sf
================================== Ai Message ===================================
Tool Calls:
  get_weather (call_9ZudXyMAdlUjptq9oMGtQo8o)
 Call ID: call_9ZudXyMAdlUjptq9oMGtQo8o
  Args:
    city: sf
================================= Tool Message ==================================
Name: get_weather

It's always sunny in sf
================================== Ai Message ===================================

The weather in San Francisco is currently sunny.
```

## View the trace in LangSmith

Now that we've ran our graph, let's head over to LangSmith and view our trace. First click into the project that you traced to (in our case the default project). You should see a run with the custom run name "agent_007".

![LangSmith Trace View](assets/d38d1f2b-0f4c-4707-b531-a3c749de987f.png)

In addition, you will be able to filter traces after the fact using the tags or metadata provided. For example,

![LangSmith Filter View](assets/410e0089-2ab8-46bb-a61a-827187fd46b3.png) 
```
---
https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/streaming.md
```md
# Stream outputs

You can [stream outputs](../concepts/streaming.md) from a LangGraph agent or workflow.

## Supported stream modes

:::python
Pass one or more of the following stream modes as a list to the @[`stream()`][CompiledStateGraph.stream] or @[`astream()`][CompiledStateGraph.astream] methods:
:::

:::js
Pass one or more of the following stream modes as a list to the @[`stream()`][CompiledStateGraph.stream] method:
:::

| Mode       | Description                                                                                                                                                                         |
| ---------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `values`   | Streams the full value of the state after each step of the graph.                                                                                                                   |
| `updates`  | Streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately. |
| `custom`   | Streams custom data from inside your graph nodes.                                                                                                                                   |
| `messages` | Streams 2-tuples (LLM token, metadata) from any graph nodes where an LLM is invoked.                                                                                                |
| `debug`    | Streams as much information as possible throughout the execution of the graph.                                                                                                      |

## Stream from an agent

### Agent progress

:::python
To stream agent progress, use the @[`stream()`][CompiledStateGraph.stream] or @[`astream()`][CompiledStateGraph.astream] methods with `stream_mode="updates"`. This emits an event after every agent step.
:::

:::js
To stream agent progress, use the @[`stream()`][CompiledStateGraph.stream] method with `streamMode: "updates"`. This emits an event after every agent step.
:::

For example, if you have an agent that calls a tool once, you should see the following updates:

- **LLM node**: AI message with tool call requests
- **Tool node**: Tool message with execution result
- **LLM node**: Final AI response

:::python
=== "Sync"

    ```python
    agent = create_react_agent(
        model="anthropic:claude-3-7-sonnet-latest",
        tools=[get_weather],
    )
    # highlight-next-line
    for chunk in agent.stream(
        {"messages": [{"role": "user", "content": "what is the weather in sf"}]},
        # highlight-next-line
        stream_mode="updates"
    ):
        print(chunk)
        print("\n")
    ```

=== "Async"

    ```python
    agent = create_react_agent(
        model="anthropic:claude-3-7-sonnet-latest",
        tools=[get_weather],
    )
    # highlight-next-line
    async for chunk in agent.astream(
        {"messages": [{"role": "user", "content": "what is the weather in sf"}]},
        # highlight-next-line
        stream_mode="updates"
    ):
        print(chunk)
        print("\n")
    ```

:::

:::js

```typescript
const agent = createReactAgent({
  llm: model,
  tools: [getWeather],
});

for await (const chunk of await agent.stream(
  { messages: [{ role: "user", content: "what is the weather in sf" }] },
  { streamMode: "updates" }
)) {
  console.log(chunk);
  console.log("\n");
}
```

:::

### LLM tokens

:::python
To stream tokens as they are produced by the LLM, use `stream_mode="messages"`:

=== "Sync"

    ```python
    agent = create_react_agent(
        model="anthropic:claude-3-7-sonnet-latest",
        tools=[get_weather],
    )
    # highlight-next-line
    for token, metadata in agent.stream(
        {"messages": [{"role": "user", "content": "what is the weather in sf"}]},
        # highlight-next-line
        stream_mode="messages"
    ):
        print("Token", token)
        print("Metadata", metadata)
        print("\n")
    ```

=== "Async"

    ```python
    agent = create_react_agent(
        model="anthropic:claude-3-7-sonnet-latest",
        tools=[get_weather],
    )
    # highlight-next-line
    async for token, metadata in agent.astream(
        {"messages": [{"role": "user", "content": "what is the weather in sf"}]},
        # highlight-next-line
        stream_mode="messages"
    ):
        print("Token", token)
        print("Metadata", metadata)
        print("\n")
    ```

:::

:::js
To stream tokens as they are produced by the LLM, use `streamMode: "messages"`:

```typescript
const agent = createReactAgent({
  llm: model,
  tools: [getWeather],
});

for await (const [token, metadata] of await agent.stream(
  { messages: [{ role: "user", content: "what is the weather in sf" }] },
  { streamMode: "messages" }
)) {
  console.log("Token", token);
  console.log("Metadata", metadata);
  console.log("\n");
}
```

:::

### Tool updates

:::python
To stream updates from tools as they are executed, you can use @[get_stream_writer][get_stream_writer].

=== "Sync"

    ```python
    # highlight-next-line
    from langgraph.config import get_stream_writer

    def get_weather(city: str) -> str:
        """Get weather for a given city."""
        # highlight-next-line
        writer = get_stream_writer()
        # stream any arbitrary data
        # highlight-next-line
        writer(f"Looking up data for city: {city}")
        return f"It's always sunny in {city}!"

    agent = create_react_agent(
        model="anthropic:claude-3-7-sonnet-latest",
        tools=[get_weather],
    )

    for chunk in agent.stream(
        {"messages": [{"role": "user", "content": "what is the weather in sf"}]},
        # highlight-next-line
        stream_mode="custom"
    ):
        print(chunk)
        print("\n")
    ```

=== "Async"

    ```python
    # highlight-next-line
    from langgraph.config import get_stream_writer

    def get_weather(city: str) -> str:
        """Get weather for a given city."""
        # highlight-next-line
        writer = get_stream_writer()
        # stream any arbitrary data
        # highlight-next-line
        writer(f"Looking up data for city: {city}")
        return f"It's always sunny in {city}!"

    agent = create_react_agent(
        model="anthropic:claude-3-7-sonnet-latest",
        tools=[get_weather],
    )

    async for chunk in agent.astream(
        {"messages": [{"role": "user", "content": "what is the weather in sf"}]},
        # highlight-next-line
        stream_mode="custom"
    ):
        print(chunk)
        print("\n")
    ```

!!! Note

      If you add `get_stream_writer` inside your tool, you won't be able to invoke the tool outside of a LangGraph execution context.

:::

:::js
To stream updates from tools as they are executed, you can use the `writer` parameter from the configuration.

```typescript
import { LangGraphRunnableConfig } from "@langchain/langgraph";

const getWeather = tool(
  async (input, config: LangGraphRunnableConfig) => {
    // Stream any arbitrary data
    config.writer?.("Looking up data for city: " + input.city);
    return `It's always sunny in ${input.city}!`;
  },
  {
    name: "get_weather",
    description: "Get weather for a given city.",
    schema: z.object({
      city: z.string().describe("The city to get weather for."),
    }),
  }
);

const agent = createReactAgent({
  llm: model,
  tools: [getWeather],
});

for await (const chunk of await agent.stream(
  { messages: [{ role: "user", content: "what is the weather in sf" }] },
  { streamMode: "custom" }
)) {
  console.log(chunk);
  console.log("\n");
}
```

!!! Note
      If you add the `writer` parameter to your tool, you won't be able to invoke the tool outside of a LangGraph execution context without providing a writer function.
:::

### Stream multiple modes

:::python
You can specify multiple streaming modes by passing stream mode as a list: `stream_mode=["updates", "messages", "custom"]`:

=== "Sync"

    ```python
    agent = create_react_agent(
        model="anthropic:claude-3-7-sonnet-latest",
        tools=[get_weather],
    )

    for stream_mode, chunk in agent.stream(
        {"messages": [{"role": "user", "content": "what is the weather in sf"}]},
        # highlight-next-line
        stream_mode=["updates", "messages", "custom"]
    ):
        print(chunk)
        print("\n")
    ```

=== "Async"

    ```python
    agent = create_react_agent(
        model="anthropic:claude-3-7-sonnet-latest",
        tools=[get_weather],
    )

    async for stream_mode, chunk in agent.astream(
        {"messages": [{"role": "user", "content": "what is the weather in sf"}]},
        # highlight-next-line
        stream_mode=["updates", "messages", "custom"]
    ):
        print(chunk)
        print("\n")
    ```

:::

:::js
You can specify multiple streaming modes by passing streamMode as an array: `streamMode: ["updates", "messages", "custom"]`:

```typescript
const agent = createReactAgent({
  llm: model,
  tools: [getWeather],
});

for await (const chunk of await agent.stream(
  { messages: [{ role: "user", content: "what is the weather in sf" }] },
  { streamMode: ["updates", "messages", "custom"] }
)) {
  console.log(chunk);
  console.log("\n");
}
```

:::

### Disable streaming

In some applications you might need to disable streaming of individual tokens for a given model. This is useful in [multi-agent](../agents/multi-agent.md) systems to control which agents stream their output.

See the [Models](../agents/models.md#disable-streaming) guide to learn how to disable streaming.

## Stream from a workflow

### Basic usage example

:::python
LangGraph graphs expose the @[`.stream()`][Pregel.stream] (sync) and @[`.astream()`][Pregel.astream] (async) methods to yield streamed outputs as iterators.

=== "Sync"

    ```python
    for chunk in graph.stream(inputs, stream_mode="updates"):
        print(chunk)
    ```

=== "Async"

    ```python
    async for chunk in graph.astream(inputs, stream_mode="updates"):
        print(chunk)
    ```

:::

:::js
LangGraph graphs expose the @[`.stream()`][Pregel.stream] method to yield streamed outputs as iterators.

```typescript
for await (const chunk of await graph.stream(inputs, {
  streamMode: "updates",
})) {
  console.log(chunk);
}
```

:::

??? example "Extended example: streaming updates"

      :::python
      ```python
      from typing import TypedDict
      from langgraph.graph import StateGraph, START, END

      class State(TypedDict):
          topic: str
          joke: str

      def refine_topic(state: State):
          return {"topic": state["topic"] + " and cats"}

      def generate_joke(state: State):
          return {"joke": f"This is a joke about {state['topic']}"}

      graph = (
          StateGraph(State)
          .add_node(refine_topic)
          .add_node(generate_joke)
          .add_edge(START, "refine_topic")
          .add_edge("refine_topic", "generate_joke")
          .add_edge("generate_joke", END)
          .compile()
      )

      # highlight-next-line
      for chunk in graph.stream( # (1)!
          {"topic": "ice cream"},
          # highlight-next-line
          stream_mode="updates", # (2)!
      ):
          print(chunk)
      ```

      1. The `stream()` method returns an iterator that yields streamed outputs.
      2. Set `stream_mode="updates"` to stream only the updates to the graph state after each node. Other stream modes are also available. See [supported stream modes](#supported-stream-modes) for details.
      :::

      :::js
      ```typescript
      import { StateGraph, START, END } from "@langchain/langgraph";
      import { z } from "zod";

      const State = z.object({
        topic: z.string(),
        joke: z.string(),
      });

      const graph = new StateGraph(State)
        .addNode("refineTopic", (state) => {
          return { topic: state.topic + " and cats" };
        })
        .addNode("generateJoke", (state) => {
          return { joke: `This is a joke about ${state.topic}` };
        })
        .addEdge(START, "refineTopic")
        .addEdge("refineTopic", "generateJoke")
        .addEdge("generateJoke", END)
        .compile();

      for await (const chunk of await graph.stream(
        { topic: "ice cream" },
        { streamMode: "updates" } // (1)!
      )) {
        console.log(chunk);
      }
      ```

      1. Set `streamMode: "updates"` to stream only the updates to the graph state after each node. Other stream modes are also available. See [supported stream modes](#supported-stream-modes) for details.
      :::

      ```output
      {'refineTopic': {'topic': 'ice cream and cats'}}
      {'generateJoke': {'joke': 'This is a joke about ice cream and cats'}}
      ```                                                                                                   |

### Stream multiple modes

:::python
You can pass a list as the `stream_mode` parameter to stream multiple modes at once.

The streamed outputs will be tuples of `(mode, chunk)` where `mode` is the name of the stream mode and `chunk` is the data streamed by that mode.

=== "Sync"

    ```python
    for mode, chunk in graph.stream(inputs, stream_mode=["updates", "custom"]):
        print(chunk)
    ```

=== "Async"

    ```python
    async for mode, chunk in graph.astream(inputs, stream_mode=["updates", "custom"]):
        print(chunk)
    ```

:::

:::js
You can pass an array as the `streamMode` parameter to stream multiple modes at once.

The streamed outputs will be tuples of `[mode, chunk]` where `mode` is the name of the stream mode and `chunk` is the data streamed by that mode.

```typescript
for await (const [mode, chunk] of await graph.stream(inputs, {
  streamMode: ["updates", "custom"],
})) {
  console.log(chunk);
}
```

:::

### Stream graph state

Use the stream modes `updates` and `values` to stream the state of the graph as it executes.

- `updates` streams the **updates** to the state after each step of the graph.
- `values` streams the **full value** of the state after each step of the graph.

:::python

```python
from typing import TypedDict
from langgraph.graph import StateGraph, START, END


class State(TypedDict):
  topic: str
  joke: str


def refine_topic(state: State):
    return {"topic": state["topic"] + " and cats"}


def generate_joke(state: State):
    return {"joke": f"This is a joke about {state['topic']}"}

graph = (
  StateGraph(State)
  .add_node(refine_topic)
  .add_node(generate_joke)
  .add_edge(START, "refine_topic")
  .add_edge("refine_topic", "generate_joke")
  .add_edge("generate_joke", END)
  .compile()
)
```

:::

:::js

```typescript
import { StateGraph, START, END } from "@langchain/langgraph";
import { z } from "zod";

const State = z.object({
  topic: z.string(),
  joke: z.string(),
});

const graph = new StateGraph(State)
  .addNode("refineTopic", (state) => {
    return { topic: state.topic + " and cats" };
  })
  .addNode("generateJoke", (state) => {
    return { joke: `This is a joke about ${state.topic}` };
  })
  .addEdge(START, "refineTopic")
  .addEdge("refineTopic", "generateJoke")
  .addEdge("generateJoke", END)
  .compile();
```

:::

=== "updates"

    Use this to stream only the **state updates** returned by the nodes after each step. The streamed outputs include the name of the node as well as the update.

    :::python
    ```python
    for chunk in graph.stream(
        {"topic": "ice cream"},
        # highlight-next-line
        stream_mode="updates",
    ):
        print(chunk)
    ```
    :::

    :::js
    ```typescript
    for await (const chunk of await graph.stream(
      { topic: "ice cream" },
      { streamMode: "updates" }
    )) {
      console.log(chunk);
    }
    ```
    :::

=== "values"

    Use this to stream the **full state** of the graph after each step.

    :::python
    ```python
    for chunk in graph.stream(
        {"topic": "ice cream"},
        # highlight-next-line
        stream_mode="values",
    ):
        print(chunk)
    ```
    :::

    :::js
    ```typescript
    for await (const chunk of await graph.stream(
      { topic: "ice cream" },
      { streamMode: "values" }
    )) {
      console.log(chunk);
    }
    ```
    :::

### Stream subgraph outputs

:::python
To include outputs from [subgraphs](../concepts/subgraphs.md) in the streamed outputs, you can set `subgraphs=True` in the `.stream()` method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.

The outputs will be streamed as tuples `(namespace, data)`, where `namespace` is a tuple with the path to the node where a subgraph is invoked, e.g. `("parent_node:<task_id>", "child_node:<task_id>")`.

```python
for chunk in graph.stream(
    {"foo": "foo"},
    # highlight-next-line
    subgraphs=True, # (1)!
    stream_mode="updates",
):
    print(chunk)
```

1. Set `subgraphs=True` to stream outputs from subgraphs.
   :::

:::js
To include outputs from [subgraphs](../concepts/subgraphs.md) in the streamed outputs, you can set `subgraphs: true` in the `.stream()` method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.

The outputs will be streamed as tuples `[namespace, data]`, where `namespace` is a tuple with the path to the node where a subgraph is invoked, e.g. `["parent_node:<task_id>", "child_node:<task_id>"]`.

```typescript
for await (const chunk of await graph.stream(
  { foo: "foo" },
  {
    subgraphs: true, // (1)!
    streamMode: "updates",
  }
)) {
  console.log(chunk);
}
```

1. Set `subgraphs: true` to stream outputs from subgraphs.
   :::

??? example "Extended example: streaming from subgraphs"

      :::python
      ```python
      from langgraph.graph import START, StateGraph
      from typing import TypedDict

      # Define subgraph
      class SubgraphState(TypedDict):
          foo: str  # note that this key is shared with the parent graph state
          bar: str

      def subgraph_node_1(state: SubgraphState):
          return {"bar": "bar"}

      def subgraph_node_2(state: SubgraphState):
          return {"foo": state["foo"] + state["bar"]}

      subgraph_builder = StateGraph(SubgraphState)
      subgraph_builder.add_node(subgraph_node_1)
      subgraph_builder.add_node(subgraph_node_2)
      subgraph_builder.add_edge(START, "subgraph_node_1")
      subgraph_builder.add_edge("subgraph_node_1", "subgraph_node_2")
      subgraph = subgraph_builder.compile()

      # Define parent graph
      class ParentState(TypedDict):
          foo: str

      def node_1(state: ParentState):
          return {"foo": "hi! " + state["foo"]}

      builder = StateGraph(ParentState)
      builder.add_node("node_1", node_1)
      builder.add_node("node_2", subgraph)
      builder.add_edge(START, "node_1")
      builder.add_edge("node_1", "node_2")
      graph = builder.compile()

      for chunk in graph.stream(
          {"foo": "foo"},
          stream_mode="updates",
          # highlight-next-line
          subgraphs=True, # (1)!
      ):
          print(chunk)
      ```

      1. Set `subgraphs=True` to stream outputs from subgraphs.
      :::

      :::js
      ```typescript
      import { StateGraph, START } from "@langchain/langgraph";
      import { z } from "zod";

      // Define subgraph
      const SubgraphState = z.object({
        foo: z.string(), // note that this key is shared with the parent graph state
        bar: z.string(),
      });

      const subgraphBuilder = new StateGraph(SubgraphState)
        .addNode("subgraphNode1", (state) => {
          return { bar: "bar" };
        })
        .addNode("subgraphNode2", (state) => {
          return { foo: state.foo + state.bar };
        })
        .addEdge(START, "subgraphNode1")
        .addEdge("subgraphNode1", "subgraphNode2");
      const subgraph = subgraphBuilder.compile();

      // Define parent graph
      const ParentState = z.object({
        foo: z.string(),
      });

      const builder = new StateGraph(ParentState)
        .addNode("node1", (state) => {
          return { foo: "hi! " + state.foo };
        })
        .addNode("node2", subgraph)
        .addEdge(START, "node1")
        .addEdge("node1", "node2");
      const graph = builder.compile();

      for await (const chunk of await graph.stream(
        { foo: "foo" },
        {
          streamMode: "updates",
          subgraphs: true, // (1)!
        }
      )) {
        console.log(chunk);
      }
      ```

      1. Set `subgraphs: true` to stream outputs from subgraphs.
      :::

      :::python
      ```
      ((), {'node_1': {'foo': 'hi! foo'}})
      (('node_2:dfddc4ba-c3c5-6887-5012-a243b5b377c2',), {'subgraph_node_1': {'bar': 'bar'}})
      (('node_2:dfddc4ba-c3c5-6887-5012-a243b5b377c2',), {'subgraph_node_2': {'foo': 'hi! foobar'}})
      ((), {'node_2': {'foo': 'hi! foobar'}})
      ```
      :::

      :::js
      ```
      [[], {'node1': {'foo': 'hi! foo'}}]
      [['node2:dfddc4ba-c3c5-6887-5012-a243b5b377c2'], {'subgraphNode1': {'bar': 'bar'}}]
      [['node2:dfddc4ba-c3c5-6887-5012-a243b5b377c2'], {'subgraphNode2': {'foo': 'hi! foobar'}}]
      [[], {'node2': {'foo': 'hi! foobar'}}]
      ```
      :::

      **Note** that we are receiving not just the node updates, but we also the namespaces which tell us what graph (or subgraph) we are streaming from.

### Debugging {#debug}

Use the `debug` streaming mode to stream as much information as possible throughout the execution of the graph. The streamed outputs include the name of the node as well as the full state.

:::python

```python
for chunk in graph.stream(
    {"topic": "ice cream"},
    # highlight-next-line
    stream_mode="debug",
):
    print(chunk)
```

:::

:::js

```typescript
for await (const chunk of await graph.stream(
  { topic: "ice cream" },
  { streamMode: "debug" }
)) {
  console.log(chunk);
}
```

:::

### LLM tokens {#messages}

Use the `messages` streaming mode to stream Large Language Model (LLM) outputs **token by token** from any part of your graph, including nodes, tools, subgraphs, or tasks.

:::python
The streamed output from [`messages` mode](#supported-stream-modes) is a tuple `(message_chunk, metadata)` where:

- `message_chunk`: the token or message segment from the LLM.
- `metadata`: a dictionary containing details about the graph node and LLM invocation.

> If your LLM is not available as a LangChain integration, you can stream its outputs using `custom` mode instead. See [use with any LLM](#use-with-any-llm) for details.

!!! warning "Manual config required for async in Python < 3.11"

    When using Python < 3.11 with async code, you must explicitly pass `RunnableConfig` to `ainvoke()` to enable proper streaming. See [Async with Python < 3.11](#async) for details or upgrade to Python 3.11+.

```python
from dataclasses import dataclass

from langchain.chat_models import init_chat_model
from langgraph.graph import StateGraph, START


@dataclass
class MyState:
    topic: str
    joke: str = ""


llm = init_chat_model(model="openai:gpt-4o-mini")

def call_model(state: MyState):
    """Call the LLM to generate a joke about a topic"""
    # highlight-next-line
    llm_response = llm.invoke( # (1)!
        [
            {"role": "user", "content": f"Generate a joke about {state.topic}"}
        ]
    )
    return {"joke": llm_response.content}

graph = (
    StateGraph(MyState)
    .add_node(call_model)
    .add_edge(START, "call_model")
    .compile()
)

for message_chunk, metadata in graph.stream( # (2)!
    {"topic": "ice cream"},
    # highlight-next-line
    stream_mode="messages",
):
    if message_chunk.content:
        print(message_chunk.content, end="|", flush=True)
```

1. Note that the message events are emitted even when the LLM is run using `.invoke` rather than `.stream`.
2. The "messages" stream mode returns an iterator of tuples `(message_chunk, metadata)` where `message_chunk` is the token streamed by the LLM and `metadata` is a dictionary with information about the graph node where the LLM was called and other information.
   :::

:::js
The streamed output from [`messages` mode](#supported-stream-modes) is a tuple `[message_chunk, metadata]` where:

- `message_chunk`: the token or message segment from the LLM.
- `metadata`: a dictionary containing details about the graph node and LLM invocation.

> If your LLM is not available as a LangChain integration, you can stream its outputs using `custom` mode instead. See [use with any LLM](#use-with-any-llm) for details.

```typescript
import { ChatOpenAI } from "@langchain/openai";
import { StateGraph, START } from "@langchain/langgraph";
import { z } from "zod";

const MyState = z.object({
  topic: z.string(),
  joke: z.string().default(""),
});

const llm = new ChatOpenAI({ model: "gpt-4o-mini" });

const callModel = async (state: z.infer<typeof MyState>) => {
  // Call the LLM to generate a joke about a topic
  const llmResponse = await llm.invoke([
    { role: "user", content: `Generate a joke about ${state.topic}` },
  ]); // (1)!
  return { joke: llmResponse.content };
};

const graph = new StateGraph(MyState)
  .addNode("callModel", callModel)
  .addEdge(START, "callModel")
  .compile();

for await (const [messageChunk, metadata] of await graph.stream(
  // (2)!
  { topic: "ice cream" },
  { streamMode: "messages" }
)) {
  if (messageChunk.content) {
    console.log(messageChunk.content + "|");
  }
}
```

1. Note that the message events are emitted even when the LLM is run using `.invoke` rather than `.stream`.
2. The "messages" stream mode returns an iterator of tuples `[messageChunk, metadata]` where `messageChunk` is the token streamed by the LLM and `metadata` is a dictionary with information about the graph node where the LLM was called and other information.
   :::

#### Filter by LLM invocation

You can associate `tags` with LLM invocations to filter the streamed tokens by LLM invocation.

:::python

```python
from langchain.chat_models import init_chat_model

llm_1 = init_chat_model(model="openai:gpt-4o-mini", tags=['joke']) # (1)!
llm_2 = init_chat_model(model="openai:gpt-4o-mini", tags=['poem']) # (2)!

graph = ... # define a graph that uses these LLMs

async for msg, metadata in graph.astream(  # (3)!
    {"topic": "cats"},
    # highlight-next-line
    stream_mode="messages",
):
    if metadata["tags"] == ["joke"]: # (4)!
        print(msg.content, end="|", flush=True)
```

1. llm_1 is tagged with "joke".
2. llm_2 is tagged with "poem".
3. The `stream_mode` is set to "messages" to stream LLM tokens. The `metadata` contains information about the LLM invocation, including the tags.
4. Filter the streamed tokens by the `tags` field in the metadata to only include the tokens from the LLM invocation with the "joke" tag.
   :::

:::js

```typescript
import { ChatOpenAI } from "@langchain/openai";

const llm1 = new ChatOpenAI({
  model: "gpt-4o-mini",
  tags: ['joke'] // (1)!
});
const llm2 = new ChatOpenAI({
  model: "gpt-4o-mini",
  tags: ['poem'] // (2)!
});

const graph = // ... define a graph that uses these LLMs

for await (const [msg, metadata] of await graph.stream( // (3)!
  { topic: "cats" },
  { streamMode: "messages" }
)) {
  if (metadata.tags?.includes("joke")) { // (4)!
    console.log(msg.content + "|");
  }
}
```

1. llm1 is tagged with "joke".
2. llm2 is tagged with "poem".
3. The `streamMode` is set to "messages" to stream LLM tokens. The `metadata` contains information about the LLM invocation, including the tags.
4. Filter the streamed tokens by the `tags` field in the metadata to only include the tokens from the LLM invocation with the "joke" tag.
   :::

??? example "Extended example: filtering by tags"

      :::python
      ```python
      from typing import TypedDict

      from langchain.chat_models import init_chat_model
      from langgraph.graph import START, StateGraph

      joke_model = init_chat_model(model="openai:gpt-4o-mini", tags=["joke"]) # (1)!
      poem_model = init_chat_model(model="openai:gpt-4o-mini", tags=["poem"]) # (2)!


      class State(TypedDict):
            topic: str
            joke: str
            poem: str


      async def call_model(state, config):
            topic = state["topic"]
            print("Writing joke...")
            # Note: Passing the config through explicitly is required for python < 3.11
            # Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks
            joke_response = await joke_model.ainvoke(
                  [{"role": "user", "content": f"Write a joke about {topic}"}],
                  config, # (3)!
            )
            print("\n\nWriting poem...")
            poem_response = await poem_model.ainvoke(
                  [{"role": "user", "content": f"Write a short poem about {topic}"}],
                  config, # (3)!
            )
            return {"joke": joke_response.content, "poem": poem_response.content}


      graph = (
            StateGraph(State)
            .add_node(call_model)
            .add_edge(START, "call_model")
            .compile()
      )

      async for msg, metadata in graph.astream(
            {"topic": "cats"},
            # highlight-next-line
            stream_mode="messages", # (4)!
      ):
          if metadata["tags"] == ["joke"]: # (4)!
              print(msg.content, end="|", flush=True)
      ```

      1. The `joke_model` is tagged with "joke".
      2. The `poem_model` is tagged with "poem".
      3. The `config` is passed through explicitly to ensure the context vars are propagated correctly. This is required for Python < 3.11 when using async code. Please see the [async section](#async) for more details.
      4. The `stream_mode` is set to "messages" to stream LLM tokens. The `metadata` contains information about the LLM invocation, including the tags.
      :::

      :::js
      ```typescript
      import { ChatOpenAI } from "@langchain/openai";
      import { StateGraph, START } from "@langchain/langgraph";
      import { z } from "zod";

      const jokeModel = new ChatOpenAI({
        model: "gpt-4o-mini",
        tags: ["joke"] // (1)!
      });
      const poemModel = new ChatOpenAI({
        model: "gpt-4o-mini",
        tags: ["poem"] // (2)!
      });

      const State = z.object({
        topic: z.string(),
        joke: z.string(),
        poem: z.string(),
      });

      const graph = new StateGraph(State)
        .addNode("callModel", (state) => {
          const topic = state.topic;
          console.log("Writing joke...");

          const jokeResponse = await jokeModel.invoke([
            { role: "user", content: `Write a joke about ${topic}` }
          ]);

          console.log("\n\nWriting poem...");
          const poemResponse = await poemModel.invoke([
            { role: "user", content: `Write a short poem about ${topic}` }
          ]);

          return {
            joke: jokeResponse.content,
            poem: poemResponse.content
          };
        })
        .addEdge(START, "callModel")
        .compile();

      for await (const [msg, metadata] of await graph.stream(
        { topic: "cats" },
        { streamMode: "messages" } // (3)!
      )) {
        if (metadata.tags?.includes("joke")) { // (4)!
          console.log(msg.content + "|");
        }
      }
      ```

      1. The `jokeModel` is tagged with "joke".
      2. The `poemModel` is tagged with "poem".
      3. The `streamMode` is set to "messages" to stream LLM tokens. The `metadata` contains information about the LLM invocation, including the tags.
      4. Filter the streamed tokens by the `tags` field in the metadata to only include the tokens from the LLM invocation with the "joke" tag.
      :::

#### Filter by node

To stream tokens only from specific nodes, use `stream_mode="messages"` and filter the outputs by the `langgraph_node` field in the streamed metadata:

:::python

```python
for msg, metadata in graph.stream( # (1)!
    inputs,
    # highlight-next-line
    stream_mode="messages",
):
    # highlight-next-line
    if msg.content and metadata["langgraph_node"] == "some_node_name": # (2)!
        ...
```

1. The "messages" stream mode returns a tuple of `(message_chunk, metadata)` where `message_chunk` is the token streamed by the LLM and `metadata` is a dictionary with information about the graph node where the LLM was called and other information.
2. Filter the streamed tokens by the `langgraph_node` field in the metadata to only include the tokens from the `write_poem` node.
   :::

:::js

```typescript
for await (const [msg, metadata] of await graph.stream(
  // (1)!
  inputs,
  { streamMode: "messages" }
)) {
  if (msg.content && metadata.langgraph_node === "some_node_name") {
    // (2)!
    // ...
  }
}
```

1. The "messages" stream mode returns a tuple of `[messageChunk, metadata]` where `messageChunk` is the token streamed by the LLM and `metadata` is a dictionary with information about the graph node where the LLM was called and other information.
2. Filter the streamed tokens by the `langgraph_node` field in the metadata to only include the tokens from the `writePoem` node.
   :::

??? example "Extended example: streaming LLM tokens from specific nodes"

      :::python
      ```python
      from typing import TypedDict
      from langgraph.graph import START, StateGraph
      from langchain_openai import ChatOpenAI

      model = ChatOpenAI(model="gpt-4o-mini")


      class State(TypedDict):
            topic: str
            joke: str
            poem: str


      def write_joke(state: State):
            topic = state["topic"]
            joke_response = model.invoke(
                  [{"role": "user", "content": f"Write a joke about {topic}"}]
            )
            return {"joke": joke_response.content}


      def write_poem(state: State):
            topic = state["topic"]
            poem_response = model.invoke(
                  [{"role": "user", "content": f"Write a short poem about {topic}"}]
            )
            return {"poem": poem_response.content}


      graph = (
            StateGraph(State)
            .add_node(write_joke)
            .add_node(write_poem)
            # write both the joke and the poem concurrently
            .add_edge(START, "write_joke")
            .add_edge(START, "write_poem")
            .compile()
      )

      # highlight-next-line
      for msg, metadata in graph.stream( # (1)!
          {"topic": "cats"},
          stream_mode="messages",
      ):
          # highlight-next-line
          if msg.content and metadata["langgraph_node"] == "write_poem": # (2)!
              print(msg.content, end="|", flush=True)
      ```

      1. The "messages" stream mode returns a tuple of `(message_chunk, metadata)` where `message_chunk` is the token streamed by the LLM and `metadata` is a dictionary with information about the graph node where the LLM was called and other information.
      2. Filter the streamed tokens by the `langgraph_node` field in the metadata to only include the tokens from the `write_poem` node.
      :::

      :::js
      ```typescript
      import { ChatOpenAI } from "@langchain/openai";
      import { StateGraph, START } from "@langchain/langgraph";
      import { z } from "zod";

      const model = new ChatOpenAI({ model: "gpt-4o-mini" });

      const State = z.object({
        topic: z.string(),
        joke: z.string(),
        poem: z.string(),
      });

      const graph = new StateGraph(State)
        .addNode("writeJoke", async (state) => {
          const topic = state.topic;
          const jokeResponse = await model.invoke([
            { role: "user", content: `Write a joke about ${topic}` }
          ]);
          return { joke: jokeResponse.content };
        })
        .addNode("writePoem", async (state) => {
          const topic = state.topic;
          const poemResponse = await model.invoke([
            { role: "user", content: `Write a short poem about ${topic}` }
          ]);
          return { poem: poemResponse.content };
        })
        // write both the joke and the poem concurrently
        .addEdge(START, "writeJoke")
        .addEdge(START, "writePoem")
        .compile();

      for await (const [msg, metadata] of await graph.stream( // (1)!
        { topic: "cats" },
        { streamMode: "messages" }
      )) {
        if (msg.content && metadata.langgraph_node === "writePoem") { // (2)!
          console.log(msg.content + "|");
        }
      }
      ```

      1. The "messages" stream mode returns a tuple of `[messageChunk, metadata]` where `messageChunk` is the token streamed by the LLM and `metadata` is a dictionary with information about the graph node where the LLM was called and other information.
      2. Filter the streamed tokens by the `langgraph_node` field in the metadata to only include the tokens from the `writePoem` node.
      :::

### Stream custom data

:::python
To send **custom user-defined data** from inside a LangGraph node or tool, follow these steps:

1. Use `get_stream_writer()` to access the stream writer and emit custom data.
2. Set `stream_mode="custom"` when calling `.stream()` or `.astream()` to get the custom data in the stream. You can combine multiple modes (e.g., `["updates", "custom"]`), but at least one must be `"custom"`.

!!! warning "No `get_stream_writer()` in async for Python < 3.11"

    In async code running on Python < 3.11, `get_stream_writer()` will not work.
    Instead, add a `writer` parameter to your node or tool and pass it manually.
    See [Async with Python < 3.11](#async) for usage examples.

=== "node"

      ```python
      from typing import TypedDict
      from langgraph.config import get_stream_writer
      from langgraph.graph import StateGraph, START

      class State(TypedDict):
          query: str
          answer: str

      def node(state: State):
          writer = get_stream_writer()  # (1)!
          writer({"custom_key": "Generating custom data inside node"}) # (2)!
          return {"answer": "some data"}

      graph = (
          StateGraph(State)
          .add_node(node)
          .add_edge(START, "node")
          .compile()
      )

      inputs = {"query": "example"}

      # Usage
      for chunk in graph.stream(inputs, stream_mode="custom"):  # (3)!
          print(chunk)
      ```

      1. Get the stream writer to send custom data.
      2. Emit a custom key-value pair (e.g., progress update).
      3. Set `stream_mode="custom"` to receive the custom data in the stream.

=== "tool"

      ```python
      from langchain_core.tools import tool
      from langgraph.config import get_stream_writer

      @tool
      def query_database(query: str) -> str:
          """Query the database."""
          writer = get_stream_writer() # (1)!
          # highlight-next-line
          writer({"data": "Retrieved 0/100 records", "type": "progress"}) # (2)!
          # perform query
          # highlight-next-line
          writer({"data": "Retrieved 100/100 records", "type": "progress"}) # (3)!
          return "some-answer"


      graph = ... # define a graph that uses this tool

      for chunk in graph.stream(inputs, stream_mode="custom"): # (4)!
          print(chunk)
      ```

      1. Access the stream writer to send custom data.
      2. Emit a custom key-value pair (e.g., progress update).
      3. Emit another custom key-value pair.
      4. Set `stream_mode="custom"` to receive the custom data in the stream.

:::

:::js
To send **custom user-defined data** from inside a LangGraph node or tool, follow these steps:

1. Use the `writer` parameter from the `LangGraphRunnableConfig` to emit custom data.
2. Set `streamMode: "custom"` when calling `.stream()` to get the custom data in the stream. You can combine multiple modes (e.g., `["updates", "custom"]`), but at least one must be `"custom"`.

=== "node"

      ```typescript
      import { StateGraph, START, LangGraphRunnableConfig } from "@langchain/langgraph";
      import { z } from "zod";

      const State = z.object({
        query: z.string(),
        answer: z.string(),
      });

      const graph = new StateGraph(State)
        .addNode("node", async (state, config) => {
          config.writer({ custom_key: "Generating custom data inside node" }); // (1)!
          return { answer: "some data" };
        })
        .addEdge(START, "node")
        .compile();

      const inputs = { query: "example" };

      // Usage
      for await (const chunk of await graph.stream(inputs, { streamMode: "custom" })) { // (2)!
        console.log(chunk);
      }
      ```

      1. Use the writer to emit a custom key-value pair (e.g., progress update).
      2. Set `streamMode: "custom"` to receive the custom data in the stream.

=== "tool"

      ```typescript
      import { tool } from "@langchain/core/tools";
      import { LangGraphRunnableConfig } from "@langchain/langgraph";
      import { z } from "zod";

      const queryDatabase = tool(
        async (input, config: LangGraphRunnableConfig) => {
          config.writer({ data: "Retrieved 0/100 records", type: "progress" }); // (1)!
          // perform query
          config.writer({ data: "Retrieved 100/100 records", type: "progress" }); // (2)!
          return "some-answer";
        },
        {
          name: "query_database",
          description: "Query the database.",
          schema: z.object({
            query: z.string().describe("The query to execute."),
          }),
        }
      );

      const graph = // ... define a graph that uses this tool

      for await (const chunk of await graph.stream(inputs, { streamMode: "custom" })) { // (3)!
        console.log(chunk);
      }
      ```

      1. Use the writer to emit a custom key-value pair (e.g., progress update).
      2. Emit another custom key-value pair.
      3. Set `streamMode: "custom"` to receive the custom data in the stream.

:::

### Use with any LLM

:::python
You can use `stream_mode="custom"` to stream data from **any LLM API**  even if that API does **not** implement the LangChain chat model interface.

This lets you integrate raw LLM clients or external services that provide their own streaming interfaces, making LangGraph highly flexible for custom setups.

```python
from langgraph.config import get_stream_writer

def call_arbitrary_model(state):
    """Example node that calls an arbitrary model and streams the output"""
    # highlight-next-line
    writer = get_stream_writer() # (1)!
    # Assume you have a streaming client that yields chunks
    for chunk in your_custom_streaming_client(state["topic"]): # (2)!
        # highlight-next-line
        writer({"custom_llm_chunk": chunk}) # (3)!
    return {"result": "completed"}

graph = (
    StateGraph(State)
    .add_node(call_arbitrary_model)
    # Add other nodes and edges as needed
    .compile()
)

for chunk in graph.stream(
    {"topic": "cats"},
    # highlight-next-line
    stream_mode="custom", # (4)!
):
    # The chunk will contain the custom data streamed from the llm
    print(chunk)
```

1. Get the stream writer to send custom data.
2. Generate LLM tokens using your custom streaming client.
3. Use the writer to send custom data to the stream.
4. Set `stream_mode="custom"` to receive the custom data in the stream.
   :::

:::js
You can use `streamMode: "custom"` to stream data from **any LLM API**  even if that API does **not** implement the LangChain chat model interface.

This lets you integrate raw LLM clients or external services that provide their own streaming interfaces, making LangGraph highly flexible for custom setups.

```typescript
import { LangGraphRunnableConfig } from "@langchain/langgraph";

const callArbitraryModel = async (
  state: any,
  config: LangGraphRunnableConfig
) => {
  // Example node that calls an arbitrary model and streams the output
  // Assume you have a streaming client that yields chunks
  for await (const chunk of yourCustomStreamingClient(state.topic)) {
    // (1)!
    config.writer({ custom_llm_chunk: chunk }); // (2)!
  }
  return { result: "completed" };
};

const graph = new StateGraph(State)
  .addNode("callArbitraryModel", callArbitraryModel)
  // Add other nodes and edges as needed
  .compile();

for await (const chunk of await graph.stream(
  { topic: "cats" },
  { streamMode: "custom" } // (3)!
)) {
  // The chunk will contain the custom data streamed from the llm
  console.log(chunk);
}
```

1. Generate LLM tokens using your custom streaming client.
2. Use the writer to send custom data to the stream.
3. Set `streamMode: "custom"` to receive the custom data in the stream.
   :::

??? example "Extended example: streaming arbitrary chat model"

      :::python
      ```python
      import operator
      import json

      from typing import TypedDict
      from typing_extensions import Annotated
      from langgraph.graph import StateGraph, START

      from openai import AsyncOpenAI

      openai_client = AsyncOpenAI()
      model_name = "gpt-4o-mini"


      async def stream_tokens(model_name: str, messages: list[dict]):
          response = await openai_client.chat.completions.create(
              messages=messages, model=model_name, stream=True
          )
          role = None
          async for chunk in response:
              delta = chunk.choices[0].delta

              if delta.role is not None:
                  role = delta.role

              if delta.content:
                  yield {"role": role, "content": delta.content}


      # this is our tool
      async def get_items(place: str) -> str:
          """Use this tool to list items one might find in a place you're asked about."""
          writer = get_stream_writer()
          response = ""
          async for msg_chunk in stream_tokens(
              model_name,
              [
                  {
                      "role": "user",
                      "content": (
                          "Can you tell me what kind of items "
                          f"i might find in the following place: '{place}'. "
                          "List at least 3 such items separating them by a comma. "
                          "And include a brief description of each item."
                      ),
                  }
              ],
          ):
              response += msg_chunk["content"]
              writer(msg_chunk)

          return response


      class State(TypedDict):
          messages: Annotated[list[dict], operator.add]


      # this is the tool-calling graph node
      async def call_tool(state: State):
          ai_message = state["messages"][-1]
          tool_call = ai_message["tool_calls"][-1]

          function_name = tool_call["function"]["name"]
          if function_name != "get_items":
              raise ValueError(f"Tool {function_name} not supported")

          function_arguments = tool_call["function"]["arguments"]
          arguments = json.loads(function_arguments)

          function_response = await get_items(**arguments)
          tool_message = {
              "tool_call_id": tool_call["id"],
              "role": "tool",
              "name": function_name,
              "content": function_response,
          }
          return {"messages": [tool_message]}


      graph = (
          StateGraph(State)
          .add_node(call_tool)
          .add_edge(START, "call_tool")
          .compile()
      )
      ```

      Let's invoke the graph with an AI message that includes a tool call:

      ```python
      inputs = {
          "messages": [
              {
                  "content": None,
                  "role": "assistant",
                  "tool_calls": [
                      {
                          "id": "1",
                          "function": {
                              "arguments": '{"place":"bedroom"}',
                              "name": "get_items",
                          },
                          "type": "function",
                      }
                  ],
              }
          ]
      }

      async for chunk in graph.astream(
          inputs,
          stream_mode="custom",
      ):
          print(chunk["content"], end="|", flush=True)
      ```
      :::

      :::js
      ```typescript
      import { StateGraph, START, LangGraphRunnableConfig } from "@langchain/langgraph";
      import { z } from "zod";
      import OpenAI from "openai";

      const openaiClient = new OpenAI();
      const modelName = "gpt-4o-mini";

      async function* streamTokens(modelName: string, messages: any[]) {
        const response = await openaiClient.chat.completions.create({
          messages,
          model: modelName,
          stream: true,
        });

        let role: string | null = null;
        for await (const chunk of response) {
          const delta = chunk.choices[0]?.delta;

          if (delta?.role) {
            role = delta.role;
          }

          if (delta?.content) {
            yield { role, content: delta.content };
          }
        }
      }

      // this is our tool
      const getItems = tool(
        async (input, config: LangGraphRunnableConfig) => {
          let response = "";
          for await (const msgChunk of streamTokens(
            modelName,
            [
              {
                role: "user",
                content: `Can you tell me what kind of items i might find in the following place: '${input.place}'. List at least 3 such items separating them by a comma. And include a brief description of each item.`,
              },
            ]
          )) {
            response += msgChunk.content;
            config.writer?.(msgChunk);
          }
          return response;
        },
        {
          name: "get_items",
          description: "Use this tool to list items one might find in a place you're asked about.",
          schema: z.object({
            place: z.string().describe("The place to look up items for."),
          }),
        }
      );

      const State = z.object({
        messages: z.array(z.any()),
      });

      const graph = new StateGraph(State)
        // this is the tool-calling graph node
        .addNode("callTool", async (state) => {
          const aiMessage = state.messages.at(-1);
          const toolCall = aiMessage.tool_calls?.at(-1);

          const functionName = toolCall?.function?.name;
          if (functionName !== "get_items") {
            throw new Error(`Tool ${functionName} not supported`);
          }

          const functionArguments = toolCall?.function?.arguments;
          const args = JSON.parse(functionArguments);

          const functionResponse = await getItems.invoke(args);
          const toolMessage = {
            tool_call_id: toolCall.id,
            role: "tool",
            name: functionName,
            content: functionResponse,
          };
          return { messages: [toolMessage] };
        })
        .addEdge(START, "callTool")
        .compile();
      ```

      Let's invoke the graph with an AI message that includes a tool call:

      ```typescript
      const inputs = {
        messages: [
          {
            content: null,
            role: "assistant",
            tool_calls: [
              {
                id: "1",
                function: {
                  arguments: '{"place":"bedroom"}',
                  name: "get_items",
                },
                type: "function",
              }
            ],
          }
        ]
      };

      for await (const chunk of await graph.stream(
        inputs,
        { streamMode: "custom" }
      )) {
        console.log(chunk.content + "|");
      }
      ```
      :::

### Disable streaming for specific chat models

If your application mixes models that support streaming with those that do not, you may need to explicitly disable streaming for
models that do not support it.

:::python
Set `disable_streaming=True` when initializing the model.

=== "init_chat_model"

      ```python
      from langchain.chat_models import init_chat_model

      model = init_chat_model(
          "anthropic:claude-3-7-sonnet-latest",
          # highlight-next-line
          disable_streaming=True # (1)!
      )
      ```

      1. Set `disable_streaming=True` to disable streaming for the chat model.

=== "chat model interface"

      ```python
      from langchain_openai import ChatOpenAI

      llm = ChatOpenAI(model="o1-preview", disable_streaming=True) # (1)!
      ```

      1. Set `disable_streaming=True` to disable streaming for the chat model.

:::

:::js
Set `streaming: false` when initializing the model.

```typescript
import { ChatOpenAI } from "@langchain/openai";

const model = new ChatOpenAI({
  model: "o1-preview",
  streaming: false, // (1)!
});
```

:::

:::python

### Async with Python < 3.11 { #async }

In Python versions < 3.11, [asyncio tasks](https://docs.python.org/3/library/asyncio-task.html#asyncio.create_task) do not support the `context` parameter.  
This limits LangGraph ability to automatically propagate context, and affects LangGraph's streaming mechanisms in two key ways:

1. You **must** explicitly pass [`RunnableConfig`](https://python.langchain.com/docs/concepts/runnables/#runnableconfig) into async LLM calls (e.g., `ainvoke()`), as callbacks are not automatically propagated.
2. You **cannot** use `get_stream_writer()` in async nodes or tools  you must pass a `writer` argument directly.

??? example "Extended example: async LLM call with manual config"

      ```python
      from typing import TypedDict
      from langgraph.graph import START, StateGraph
      from langchain.chat_models import init_chat_model

      llm = init_chat_model(model="openai:gpt-4o-mini")

      class State(TypedDict):
          topic: str
          joke: str

      async def call_model(state, config): # (1)!
          topic = state["topic"]
          print("Generating joke...")
          joke_response = await llm.ainvoke(
              [{"role": "user", "content": f"Write a joke about {topic}"}],
              # highlight-next-line
              config, # (2)!
          )
          return {"joke": joke_response.content}

      graph = (
          StateGraph(State)
          .add_node(call_model)
          .add_edge(START, "call_model")
          .compile()
      )

      async for chunk, metadata in graph.astream(
          {"topic": "ice cream"},
          # highlight-next-line
          stream_mode="messages", # (3)!
      ):
          if chunk.content:
              print(chunk.content, end="|", flush=True)
      ```

      1. Accept `config` as an argument in the async node function.
      2. Pass `config` to `llm.ainvoke()` to ensure proper context propagation.
      3. Set `stream_mode="messages"` to stream LLM tokens.

??? example "Extended example: async custom streaming with stream writer"

      ```python
      from typing import TypedDict
      from langgraph.types import StreamWriter

      class State(TypedDict):
            topic: str
            joke: str

      # highlight-next-line
      async def generate_joke(state: State, writer: StreamWriter): # (1)!
            writer({"custom_key": "Streaming custom data while generating a joke"})
            return {"joke": f"This is a joke about {state['topic']}"}

      graph = (
            StateGraph(State)
            .add_node(generate_joke)
            .add_edge(START, "generate_joke")
            .compile()
      )

      async for chunk in graph.astream(
            {"topic": "ice cream"},
            # highlight-next-line
            stream_mode="custom", # (2)!
      ):
            print(chunk)
      ```

      1. Add `writer` as an argument in the function signature of the async node or tool. LangGraph will automatically pass the stream writer to the function.
      2. Set `stream_mode="custom"` to receive the custom data in the stream.

:::

```
---
https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/subgraph.md
```md
# Use subgraphs

This guide explains the mechanics of using [subgraphs](../concepts/subgraphs.md). A common application of subgraphs is to build [multi-agent](../concepts/multi_agent.md) systems.

When adding subgraphs, you need to define how the parent graph and the subgraph communicate:

* [Shared state schemas](#shared-state-schemas)  parent and subgraph have **shared state keys** in their state [schemas](../concepts/low_level.md#state)
* [Different state schemas](#different-state-schemas)  **no shared state keys** in parent and subgraph [schemas](../concepts/low_level.md#state)

## Setup

:::python
```bash
pip install -U langgraph
```
:::

:::js
```bash
npm install @langchain/langgraph
```
:::

!!! tip "Set up LangSmith for LangGraph development"

    Sign up for [LangSmith](https://smith.langchain.com) to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph  read more about how to get started [here](https://docs.smith.langchain.com).

## Shared state schemas

A common case is for the parent graph and subgraph to communicate over a shared state key (channel) in the [schema](../concepts/low_level.md#state). For example, in [multi-agent](../concepts/multi_agent.md) systems, the agents often communicate over a shared [messages](https://langchain-ai.github.io/langgraph/concepts/low_level.md#why-use-messages) key.

If your subgraph shares state keys with the parent graph, you can follow these steps to add it to your graph:

:::python
1. Define the subgraph workflow (`subgraph_builder` in the example below) and compile it
2. Pass compiled subgraph to the `.add_node` method when defining the parent graph workflow

```python
from typing_extensions import TypedDict
from langgraph.graph.state import StateGraph, START

class State(TypedDict):
    foo: str

# Subgraph

def subgraph_node_1(state: State):
    return {"foo": "hi! " + state["foo"]}

subgraph_builder = StateGraph(State)
subgraph_builder.add_node(subgraph_node_1)
subgraph_builder.add_edge(START, "subgraph_node_1")
subgraph = subgraph_builder.compile()

# Parent graph

builder = StateGraph(State)
builder.add_node("node_1", subgraph)
builder.add_edge(START, "node_1")
graph = builder.compile()
```
:::

:::js
1. Define the subgraph workflow (`subgraphBuilder` in the example below) and compile it
2. Pass compiled subgraph to the `.addNode` method when defining the parent graph workflow

```typescript
import { StateGraph, START } from "@langchain/langgraph";
import { z } from "zod";

const State = z.object({
  foo: z.string(),
});

// Subgraph
const subgraphBuilder = new StateGraph(State)
  .addNode("subgraphNode1", (state) => {
    return { foo: "hi! " + state.foo };
  })
  .addEdge(START, "subgraphNode1");

const subgraph = subgraphBuilder.compile();

// Parent graph
const builder = new StateGraph(State)
  .addNode("node1", subgraph)
  .addEdge(START, "node1");

const graph = builder.compile();
```
:::

??? example "Full example: shared state schemas"

    :::python
    ```python
    from typing_extensions import TypedDict
    from langgraph.graph.state import StateGraph, START

    # Define subgraph
    class SubgraphState(TypedDict):
        foo: str  # (1)! 
        bar: str  # (2)!
    
    def subgraph_node_1(state: SubgraphState):
        return {"bar": "bar"}
    
    def subgraph_node_2(state: SubgraphState):
        # note that this node is using a state key ('bar') that is only available in the subgraph
        # and is sending update on the shared state key ('foo')
        return {"foo": state["foo"] + state["bar"]}
    
    subgraph_builder = StateGraph(SubgraphState)
    subgraph_builder.add_node(subgraph_node_1)
    subgraph_builder.add_node(subgraph_node_2)
    subgraph_builder.add_edge(START, "subgraph_node_1")
    subgraph_builder.add_edge("subgraph_node_1", "subgraph_node_2")
    subgraph = subgraph_builder.compile()
    
    # Define parent graph
    class ParentState(TypedDict):
        foo: str
    
    def node_1(state: ParentState):
        return {"foo": "hi! " + state["foo"]}
    
    builder = StateGraph(ParentState)
    builder.add_node("node_1", node_1)
    builder.add_node("node_2", subgraph)
    builder.add_edge(START, "node_1")
    builder.add_edge("node_1", "node_2")
    graph = builder.compile()
    
    for chunk in graph.stream({"foo": "foo"}):
        print(chunk)
    ```

    1. This key is shared with the parent graph state
    2. This key is private to the `SubgraphState` and is not visible to the parent graph
    
    ```
    {'node_1': {'foo': 'hi! foo'}}
    {'node_2': {'foo': 'hi! foobar'}}
    ```
    :::

    :::js
    ```typescript
    import { StateGraph, START } from "@langchain/langgraph";
    import { z } from "zod";

    // Define subgraph
    const SubgraphState = z.object({
      foo: z.string(),  // (1)! 
      bar: z.string(),  // (2)!
    });
    
    const subgraphBuilder = new StateGraph(SubgraphState)
      .addNode("subgraphNode1", (state) => {
        return { bar: "bar" };
      })
      .addNode("subgraphNode2", (state) => {
        // note that this node is using a state key ('bar') that is only available in the subgraph
        // and is sending update on the shared state key ('foo')
        return { foo: state.foo + state.bar };
      })
      .addEdge(START, "subgraphNode1")
      .addEdge("subgraphNode1", "subgraphNode2");
    
    const subgraph = subgraphBuilder.compile();
    
    // Define parent graph
    const ParentState = z.object({
      foo: z.string(),
    });
    
    const builder = new StateGraph(ParentState)
      .addNode("node1", (state) => {
        return { foo: "hi! " + state.foo };
      })
      .addNode("node2", subgraph)
      .addEdge(START, "node1")
      .addEdge("node1", "node2");
    
    const graph = builder.compile();
    
    for await (const chunk of await graph.stream({ foo: "foo" })) {
      console.log(chunk);
    }
    ```

    3. This key is shared with the parent graph state
    4. This key is private to the `SubgraphState` and is not visible to the parent graph
    
    ```
    { node1: { foo: 'hi! foo' } }
    { node2: { foo: 'hi! foobar' } }
    ```
    :::

## Different state schemas

For more complex systems you might want to define subgraphs that have a **completely different schema** from the parent graph (no shared keys). For example, you might want to keep a private message history for each of the agents in a [multi-agent](../concepts/multi_agent.md) system.

If that's the case for your application, you need to define a node **function that invokes the subgraph**. This function needs to transform the input (parent) state to the subgraph state before invoking the subgraph, and transform the results back to the parent state before returning the state update from the node.

:::python
```python
from typing_extensions import TypedDict
from langgraph.graph.state import StateGraph, START

class SubgraphState(TypedDict):
    bar: str

# Subgraph

def subgraph_node_1(state: SubgraphState):
    return {"bar": "hi! " + state["bar"]}

subgraph_builder = StateGraph(SubgraphState)
subgraph_builder.add_node(subgraph_node_1)
subgraph_builder.add_edge(START, "subgraph_node_1")
subgraph = subgraph_builder.compile()

# Parent graph

class State(TypedDict):
    foo: str

def call_subgraph(state: State):
    subgraph_output = subgraph.invoke({"bar": state["foo"]})  # (1)!
    return {"foo": subgraph_output["bar"]}  # (2)!

builder = StateGraph(State)
builder.add_node("node_1", call_subgraph)
builder.add_edge(START, "node_1")
graph = builder.compile()
```

1. Transform the state to the subgraph state
2. Transform response back to the parent state
:::

:::js
```typescript
import { StateGraph, START } from "@langchain/langgraph";
import { z } from "zod";

const SubgraphState = z.object({
  bar: z.string(),
});

// Subgraph
const subgraphBuilder = new StateGraph(SubgraphState)
  .addNode("subgraphNode1", (state) => {
    return { bar: "hi! " + state.bar };
  })
  .addEdge(START, "subgraphNode1");

const subgraph = subgraphBuilder.compile();

// Parent graph
const State = z.object({
  foo: z.string(),
});

const builder = new StateGraph(State)
  .addNode("node1", async (state) => {
    const subgraphOutput = await subgraph.invoke({ bar: state.foo }); // (1)!
    return { foo: subgraphOutput.bar }; // (2)!
  })
  .addEdge(START, "node1");

const graph = builder.compile();
```

1. Transform the state to the subgraph state
2. Transform response back to the parent state
:::

??? example "Full example: different state schemas"

    :::python
    ```python
    from typing_extensions import TypedDict
    from langgraph.graph.state import StateGraph, START

    # Define subgraph
    class SubgraphState(TypedDict):
        # note that none of these keys are shared with the parent graph state
        bar: str
        baz: str
    
    def subgraph_node_1(state: SubgraphState):
        return {"baz": "baz"}
    
    def subgraph_node_2(state: SubgraphState):
        return {"bar": state["bar"] + state["baz"]}
    
    subgraph_builder = StateGraph(SubgraphState)
    subgraph_builder.add_node(subgraph_node_1)
    subgraph_builder.add_node(subgraph_node_2)
    subgraph_builder.add_edge(START, "subgraph_node_1")
    subgraph_builder.add_edge("subgraph_node_1", "subgraph_node_2")
    subgraph = subgraph_builder.compile()
    
    # Define parent graph
    class ParentState(TypedDict):
        foo: str
    
    def node_1(state: ParentState):
        return {"foo": "hi! " + state["foo"]}
    
    def node_2(state: ParentState):
        response = subgraph.invoke({"bar": state["foo"]})  # (1)!
        return {"foo": response["bar"]}  # (2)!
    
    
    builder = StateGraph(ParentState)
    builder.add_node("node_1", node_1)
    builder.add_node("node_2", node_2)
    builder.add_edge(START, "node_1")
    builder.add_edge("node_1", "node_2")
    graph = builder.compile()
    
    for chunk in graph.stream({"foo": "foo"}, subgraphs=True):
        print(chunk)
    ```

    1. Transform the state to the subgraph state
    2. Transform response back to the parent state

    ```
    ((), {'node_1': {'foo': 'hi! foo'}})
    (('node_2:9c36dd0f-151a-cb42-cbad-fa2f851f9ab7',), {'grandchild_1': {'my_grandchild_key': 'hi Bob, how are you'}})
    (('node_2:9c36dd0f-151a-cb42-cbad-fa2f851f9ab7',), {'grandchild_2': {'bar': 'hi! foobaz'}})
    ((), {'node_2': {'foo': 'hi! foobaz'}})
    ```
    :::

    :::js
    ```typescript
    import { StateGraph, START } from "@langchain/langgraph";
    import { z } from "zod";

    // Define subgraph
    const SubgraphState = z.object({
      // note that none of these keys are shared with the parent graph state
      bar: z.string(),
      baz: z.string(),
    });
    
    const subgraphBuilder = new StateGraph(SubgraphState)
      .addNode("subgraphNode1", (state) => {
        return { baz: "baz" };
      })
      .addNode("subgraphNode2", (state) => {
        return { bar: state.bar + state.baz };
      })
      .addEdge(START, "subgraphNode1")
      .addEdge("subgraphNode1", "subgraphNode2");
    
    const subgraph = subgraphBuilder.compile();
    
    // Define parent graph
    const ParentState = z.object({
      foo: z.string(),
    });
    
    const builder = new StateGraph(ParentState)
      .addNode("node1", (state) => {
        return { foo: "hi! " + state.foo };
      })
      .addNode("node2", async (state) => {
        const response = await subgraph.invoke({ bar: state.foo }); // (1)!
        return { foo: response.bar }; // (2)!
      })
      .addEdge(START, "node1")
      .addEdge("node1", "node2");
    
    const graph = builder.compile();
    
    for await (const chunk of await graph.stream(
      { foo: "foo" }, 
      { subgraphs: true }
    )) {
      console.log(chunk);
    }
    ```

    3. Transform the state to the subgraph state
    4. Transform response back to the parent state

    ```
    [[], { node1: { foo: 'hi! foo' } }]
    [['node2:9c36dd0f-151a-cb42-cbad-fa2f851f9ab7'], { subgraphNode1: { baz: 'baz' } }]
    [['node2:9c36dd0f-151a-cb42-cbad-fa2f851f9ab7'], { subgraphNode2: { bar: 'hi! foobaz' } }]
    [[], { node2: { foo: 'hi! foobaz' } }]
    ```
    :::

??? example "Full example: different state schemas (two levels of subgraphs)"

    This is an example with two levels of subgraphs: parent -> child -> grandchild.

    :::python
    ```python
    # Grandchild graph
    from typing_extensions import TypedDict
    from langgraph.graph.state import StateGraph, START, END
    
    class GrandChildState(TypedDict):
        my_grandchild_key: str
    
    def grandchild_1(state: GrandChildState) -> GrandChildState:
        # NOTE: child or parent keys will not be accessible here
        return {"my_grandchild_key": state["my_grandchild_key"] + ", how are you"}
    
    
    grandchild = StateGraph(GrandChildState)
    grandchild.add_node("grandchild_1", grandchild_1)
    
    grandchild.add_edge(START, "grandchild_1")
    grandchild.add_edge("grandchild_1", END)
    
    grandchild_graph = grandchild.compile()
    
    # Child graph
    class ChildState(TypedDict):
        my_child_key: str
    
    def call_grandchild_graph(state: ChildState) -> ChildState:
        # NOTE: parent or grandchild keys won't be accessible here
        grandchild_graph_input = {"my_grandchild_key": state["my_child_key"]}  # (1)!
        grandchild_graph_output = grandchild_graph.invoke(grandchild_graph_input)
        return {"my_child_key": grandchild_graph_output["my_grandchild_key"] + " today?"}  # (2)!
    
    child = StateGraph(ChildState)
    child.add_node("child_1", call_grandchild_graph)  # (3)!
    child.add_edge(START, "child_1")
    child.add_edge("child_1", END)
    child_graph = child.compile()
    
    # Parent graph
    class ParentState(TypedDict):
        my_key: str
    
    def parent_1(state: ParentState) -> ParentState:
        # NOTE: child or grandchild keys won't be accessible here
        return {"my_key": "hi " + state["my_key"]}
    
    def parent_2(state: ParentState) -> ParentState:
        return {"my_key": state["my_key"] + " bye!"}
    
    def call_child_graph(state: ParentState) -> ParentState:
        child_graph_input = {"my_child_key": state["my_key"]}  # (4)!
        child_graph_output = child_graph.invoke(child_graph_input)
        return {"my_key": child_graph_output["my_child_key"]}  # (5)!
    
    parent = StateGraph(ParentState)
    parent.add_node("parent_1", parent_1)
    parent.add_node("child", call_child_graph)  # (6)!
    parent.add_node("parent_2", parent_2)
    
    parent.add_edge(START, "parent_1")
    parent.add_edge("parent_1", "child")
    parent.add_edge("child", "parent_2")
    parent.add_edge("parent_2", END)
    
    parent_graph = parent.compile()
    
    for chunk in parent_graph.stream({"my_key": "Bob"}, subgraphs=True):
        print(chunk)
    ```

    1. We're transforming the state from the child state channels (`my_child_key`) to the child state channels (`my_grandchild_key`)
    2. We're transforming the state from the grandchild state channels (`my_grandchild_key`) back to the child state channels (`my_child_key`)
    3. We're passing a function here instead of just compiled graph (`grandchild_graph`)
    4. We're transforming the state from the parent state channels (`my_key`) to the child state channels (`my_child_key`)
    5. We're transforming the state from the child state channels (`my_child_key`) back to the parent state channels (`my_key`)
    6. We're passing a function here instead of just a compiled graph (`child_graph`)

    ```
    ((), {'parent_1': {'my_key': 'hi Bob'}})
    (('child:2e26e9ce-602f-862c-aa66-1ea5a4655e3b', 'child_1:781bb3b1-3971-84ce-810b-acf819a03f9c'), {'grandchild_1': {'my_grandchild_key': 'hi Bob, how are you'}})
    (('child:2e26e9ce-602f-862c-aa66-1ea5a4655e3b',), {'child_1': {'my_child_key': 'hi Bob, how are you today?'}})
    ((), {'child': {'my_key': 'hi Bob, how are you today?'}})
    ((), {'parent_2': {'my_key': 'hi Bob, how are you today? bye!'}})
    ```
    :::

    :::js
    ```typescript
    import { StateGraph, START, END } from "@langchain/langgraph";
    import { z } from "zod";

    // Grandchild graph
    const GrandChildState = z.object({
      myGrandchildKey: z.string(),
    });
    
    const grandchild = new StateGraph(GrandChildState)
      .addNode("grandchild1", (state) => {
        // NOTE: child or parent keys will not be accessible here
        return { myGrandchildKey: state.myGrandchildKey + ", how are you" };
      })
      .addEdge(START, "grandchild1")
      .addEdge("grandchild1", END);
    
    const grandchildGraph = grandchild.compile();
    
    // Child graph
    const ChildState = z.object({
      myChildKey: z.string(),
    });
    
    const child = new StateGraph(ChildState)
      .addNode("child1", async (state) => {
        // NOTE: parent or grandchild keys won't be accessible here
        const grandchildGraphInput = { myGrandchildKey: state.myChildKey }; // (1)!
        const grandchildGraphOutput = await grandchildGraph.invoke(grandchildGraphInput);
        return { myChildKey: grandchildGraphOutput.myGrandchildKey + " today?" }; // (2)!
      }) // (3)!
      .addEdge(START, "child1")
      .addEdge("child1", END);
    
    const childGraph = child.compile();
    
    // Parent graph
    const ParentState = z.object({
      myKey: z.string(),
    });
    
    const parent = new StateGraph(ParentState)
      .addNode("parent1", (state) => {
        // NOTE: child or grandchild keys won't be accessible here
        return { myKey: "hi " + state.myKey };
      })
      .addNode("child", async (state) => {
        const childGraphInput = { myChildKey: state.myKey }; // (4)!
        const childGraphOutput = await childGraph.invoke(childGraphInput);
        return { myKey: childGraphOutput.myChildKey }; // (5)!
      }) // (6)!
      .addNode("parent2", (state) => {
        return { myKey: state.myKey + " bye!" };
      })
      .addEdge(START, "parent1")
      .addEdge("parent1", "child")
      .addEdge("child", "parent2")
      .addEdge("parent2", END);
    
    const parentGraph = parent.compile();
    
    for await (const chunk of await parentGraph.stream(
      { myKey: "Bob" }, 
      { subgraphs: true }
    )) {
      console.log(chunk);
    }
    ```

    7. We're transforming the state from the child state channels (`myChildKey`) to the grandchild state channels (`myGrandchildKey`)
    8. We're transforming the state from the grandchild state channels (`myGrandchildKey`) back to the child state channels (`myChildKey`)
    9. We're passing a function here instead of just compiled graph (`grandchildGraph`)
    10. We're transforming the state from the parent state channels (`myKey`) to the child state channels (`myChildKey`)
    11. We're transforming the state from the child state channels (`myChildKey`) back to the parent state channels (`myKey`)
    12. We're passing a function here instead of just a compiled graph (`childGraph`)

    ```
    [[], { parent1: { myKey: 'hi Bob' } }]
    [['child:2e26e9ce-602f-862c-aa66-1ea5a4655e3b', 'child1:781bb3b1-3971-84ce-810b-acf819a03f9c'], { grandchild1: { myGrandchildKey: 'hi Bob, how are you' } }]
    [['child:2e26e9ce-602f-862c-aa66-1ea5a4655e3b'], { child1: { myChildKey: 'hi Bob, how are you today?' } }]
    [[], { child: { myKey: 'hi Bob, how are you today?' } }]
    [[], { parent2: { myKey: 'hi Bob, how are you today? bye!' } }]
    ```
    :::

## Add persistence 

You only need to **provide the checkpointer when compiling the parent graph**. LangGraph will automatically propagate the checkpointer to the child subgraphs.

:::python
```python
from langgraph.graph import START, StateGraph
from langgraph.checkpoint.memory import MemorySaver
from typing_extensions import TypedDict

class State(TypedDict):
    foo: str

# Subgraph

def subgraph_node_1(state: State):
    return {"foo": state["foo"] + "bar"}

subgraph_builder = StateGraph(State)
subgraph_builder.add_node(subgraph_node_1)
subgraph_builder.add_edge(START, "subgraph_node_1")
subgraph = subgraph_builder.compile()

# Parent graph

builder = StateGraph(State)
builder.add_node("node_1", subgraph)
builder.add_edge(START, "node_1")

checkpointer = MemorySaver()
graph = builder.compile(checkpointer=checkpointer)
```    
:::

:::js
```typescript
import { StateGraph, START, MemorySaver } from "@langchain/langgraph";
import { z } from "zod";

const State = z.object({
  foo: z.string(),
});

// Subgraph
const subgraphBuilder = new StateGraph(State)
  .addNode("subgraphNode1", (state) => {
    return { foo: state.foo + "bar" };
  })
  .addEdge(START, "subgraphNode1");

const subgraph = subgraphBuilder.compile();

// Parent graph
const builder = new StateGraph(State)
  .addNode("node1", subgraph)
  .addEdge(START, "node1");

const checkpointer = new MemorySaver();
const graph = builder.compile({ checkpointer });
```    
:::

If you want the subgraph to **have its own memory**, you can compile it with the appropriate checkpointer option. This is useful in [multi-agent](../concepts/multi_agent.md) systems, if you want agents to keep track of their internal message histories:

:::python
```python
subgraph_builder = StateGraph(...)
subgraph = subgraph_builder.compile(checkpointer=True)
```
:::

:::js
```typescript
const subgraphBuilder = new StateGraph(...)
const subgraph = subgraphBuilder.compile({ checkpointer: true });
```
:::

## View subgraph state

When you enable [persistence](../concepts/persistence.md), you can [inspect the graph state](../concepts/persistence.md#checkpoints) (checkpoint) via the appropriate method. To view the subgraph state, you can use the subgraphs option.

:::python
You can inspect the graph state via `graph.get_state(config)`. To view the subgraph state, you can use `graph.get_state(config, subgraphs=True)`.
:::

:::js
You can inspect the graph state via `graph.getState(config)`. To view the subgraph state, you can use `graph.getState(config, { subgraphs: true })`.
:::

!!! important "Available **only** when interrupted"

    Subgraph state can only be viewed **when the subgraph is interrupted**. Once you resume the graph, you won't be able to access the subgraph state.

??? example "View interrupted subgraph state"

    :::python
    ```python
    from langgraph.graph import START, StateGraph
    from langgraph.checkpoint.memory import MemorySaver
    from langgraph.types import interrupt, Command
    from typing_extensions import TypedDict
    
    class State(TypedDict):
        foo: str
    
    # Subgraph
    
    def subgraph_node_1(state: State):
        value = interrupt("Provide value:")
        return {"foo": state["foo"] + value}
    
    subgraph_builder = StateGraph(State)
    subgraph_builder.add_node(subgraph_node_1)
    subgraph_builder.add_edge(START, "subgraph_node_1")
    
    subgraph = subgraph_builder.compile()
    
    # Parent graph
        
    builder = StateGraph(State)
    builder.add_node("node_1", subgraph)
    builder.add_edge(START, "node_1")
    
    checkpointer = MemorySaver()
    graph = builder.compile(checkpointer=checkpointer)
    
    config = {"configurable": {"thread_id": "1"}}
    
    graph.invoke({"foo": ""}, config)
    parent_state = graph.get_state(config)
    subgraph_state = graph.get_state(config, subgraphs=True).tasks[0].state  # (1)!
    
    # resume the subgraph
    graph.invoke(Command(resume="bar"), config)
    ```
    
    1. This will be available only when the subgraph is interrupted. Once you resume the graph, you won't be able to access the subgraph state.
    :::

    :::js
    ```typescript
    import { StateGraph, START, MemorySaver, interrupt, Command } from "@langchain/langgraph";
    import { z } from "zod";
    
    const State = z.object({
      foo: z.string(),
    });
    
    // Subgraph
    const subgraphBuilder = new StateGraph(State)
      .addNode("subgraphNode1", (state) => {
        const value = interrupt("Provide value:");
        return { foo: state.foo + value };
      })
      .addEdge(START, "subgraphNode1");
    
    const subgraph = subgraphBuilder.compile();
    
    // Parent graph
    const builder = new StateGraph(State)
      .addNode("node1", subgraph)
      .addEdge(START, "node1");
    
    const checkpointer = new MemorySaver();
    const graph = builder.compile({ checkpointer });
    
    const config = { configurable: { thread_id: "1" } };
    
    await graph.invoke({ foo: "" }, config);
    const parentState = await graph.getState(config);
    const subgraphState = (await graph.getState(config, { subgraphs: true })).tasks[0].state; // (1)!
    
    // resume the subgraph
    await graph.invoke(new Command({ resume: "bar" }), config);
    ```
    
    2. This will be available only when the subgraph is interrupted. Once you resume the graph, you won't be able to access the subgraph state.
    :::

## Stream subgraph outputs

To include outputs from subgraphs in the streamed outputs, you can set the subgraphs option in the stream method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.

:::python
```python
for chunk in graph.stream(
    {"foo": "foo"},
    subgraphs=True, # (1)!
    stream_mode="updates",
):
    print(chunk)
```

1. Set `subgraphs=True` to stream outputs from subgraphs.
:::

:::js
```typescript
for await (const chunk of await graph.stream(
  { foo: "foo" },
  {
    subgraphs: true, // (1)!
    streamMode: "updates",
  }
)) {
  console.log(chunk);
}
```

1. Set `subgraphs: true` to stream outputs from subgraphs.
:::

??? example "Stream from subgraphs"

    :::python
    ```python
    from typing_extensions import TypedDict
    from langgraph.graph.state import StateGraph, START

    # Define subgraph
    class SubgraphState(TypedDict):
        foo: str
        bar: str
    
    def subgraph_node_1(state: SubgraphState):
        return {"bar": "bar"}
    
    def subgraph_node_2(state: SubgraphState):
        # note that this node is using a state key ('bar') that is only available in the subgraph
        # and is sending update on the shared state key ('foo')
        return {"foo": state["foo"] + state["bar"]}
    
    subgraph_builder = StateGraph(SubgraphState)
    subgraph_builder.add_node(subgraph_node_1)
    subgraph_builder.add_node(subgraph_node_2)
    subgraph_builder.add_edge(START, "subgraph_node_1")
    subgraph_builder.add_edge("subgraph_node_1", "subgraph_node_2")
    subgraph = subgraph_builder.compile()
    
    # Define parent graph
    class ParentState(TypedDict):
        foo: str
    
    def node_1(state: ParentState):
        return {"foo": "hi! " + state["foo"]}
    
    builder = StateGraph(ParentState)
    builder.add_node("node_1", node_1)
    builder.add_node("node_2", subgraph)
    builder.add_edge(START, "node_1")
    builder.add_edge("node_1", "node_2")
    graph = builder.compile()

    for chunk in graph.stream(
        {"foo": "foo"},
        stream_mode="updates",
        subgraphs=True, # (1)!
    ):
        print(chunk)
    ```
  
    1. Set `subgraphs=True` to stream outputs from subgraphs.

    ```
    ((), {'node_1': {'foo': 'hi! foo'}})
    (('node_2:e58e5673-a661-ebb0-70d4-e298a7fc28b7',), {'subgraph_node_1': {'bar': 'bar'}})
    (('node_2:e58e5673-a661-ebb0-70d4-e298a7fc28b7',), {'subgraph_node_2': {'foo': 'hi! foobar'}})
    ((), {'node_2': {'foo': 'hi! foobar'}})
    ```
    :::

    :::js
    ```typescript
    import { StateGraph, START } from "@langchain/langgraph";
    import { z } from "zod";

    // Define subgraph
    const SubgraphState = z.object({
      foo: z.string(),
      bar: z.string(),
    });
    
    const subgraphBuilder = new StateGraph(SubgraphState)
      .addNode("subgraphNode1", (state) => {
        return { bar: "bar" };
      })
      .addNode("subgraphNode2", (state) => {
        // note that this node is using a state key ('bar') that is only available in the subgraph
        // and is sending update on the shared state key ('foo')
        return { foo: state.foo + state.bar };
      })
      .addEdge(START, "subgraphNode1")
      .addEdge("subgraphNode1", "subgraphNode2");
    
    const subgraph = subgraphBuilder.compile();
    
    // Define parent graph
    const ParentState = z.object({
      foo: z.string(),
    });
    
    const builder = new StateGraph(ParentState)
      .addNode("node1", (state) => {
        return { foo: "hi! " + state.foo };
      })
      .addNode("node2", subgraph)
      .addEdge(START, "node1")
      .addEdge("node1", "node2");
    
    const graph = builder.compile();

    for await (const chunk of await graph.stream(
      { foo: "foo" },
      {
        streamMode: "updates",
        subgraphs: true, // (1)!
      }
    )) {
      console.log(chunk);
    }
    ```
  
    2. Set `subgraphs: true` to stream outputs from subgraphs.

    ```
    [[], { node1: { foo: 'hi! foo' } }]
    [['node2:e58e5673-a661-ebb0-70d4-e298a7fc28b7'], { subgraphNode1: { bar: 'bar' } }]
    [['node2:e58e5673-a661-ebb0-70d4-e298a7fc28b7'], { subgraphNode2: { foo: 'hi! foobar' } }]
    [[], { node2: { foo: 'hi! foobar' } }]
    ```
    :::
```
---
https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/tool-calling.md
```md
# Call tools

[Tools](../concepts/tools.md) encapsulate a callable function and its input schema. These can be passed to compatible chat models, allowing the model to decide whether to invoke a tool and determine the appropriate arguments.

You can [define your own tools](#define-a-tool) or use [prebuilt tools](#prebuilt-tools)

## Define a tool

:::python
Define a basic tool with the [@tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) decorator:

```python
from langchain_core.tools import tool

# highlight-next-line
@tool
def multiply(a: int, b: int) -> int:
    """Multiply two numbers."""
    return a * b
```

:::

:::js
Define a basic tool with the [tool](https://js.langchain.com/docs/api/core/tools/classes/tool.html) function:

```typescript
import { tool } from "@langchain/core/tools";
import { z } from "zod";

// highlight-next-line
const multiply = tool(
  (input) => {
    return input.a * input.b;
  },
  {
    name: "multiply",
    description: "Multiply two numbers.",
    schema: z.object({
      a: z.number().describe("First operand"),
      b: z.number().describe("Second operand"),
    }),
  }
);
```

:::

## Run a tool

Tools conform to the [Runnable interface](https://python.langchain.com/docs/concepts/runnables/), which means you can run a tool using the `invoke` method:

:::python

```python
multiply.invoke({"a": 6, "b": 7})  # returns 42
```

:::

:::js

```typescript
await multiply.invoke({ a: 6, b: 7 }); // returns 42
```

:::

If the tool is invoked with `type="tool_call"`, it will return a [ToolMessage](https://python.langchain.com/docs/concepts/messages/#toolmessage):

:::python

```python
tool_call = {
    "type": "tool_call",
    "id": "1",
    "args": {"a": 42, "b": 7}
}
multiply.invoke(tool_call) # returns a ToolMessage object
```

Output:

```pycon
ToolMessage(content='294', name='multiply', tool_call_id='1')
```

:::

:::js

```typescript
const toolCall = {
  type: "tool_call",
  id: "1",
  name: "multiply",
  args: { a: 42, b: 7 },
};
await multiply.invoke(toolCall); // returns a ToolMessage object
```

Output:

```
ToolMessage {
  content: "294",
  name: "multiply",
  tool_call_id: "1"
}
```

:::

## Use in an agent

:::python
To create a tool-calling agent, you can use the prebuilt @[create_react_agent][create_react_agent]:

```python
from langchain_core.tools import tool
# highlight-next-line
from langgraph.prebuilt import create_react_agent

@tool
def multiply(a: int, b: int) -> int:
    """Multiply two numbers."""
    return a * b

# highlight-next-line
agent = create_react_agent(
    model="anthropic:claude-3-7-sonnet",
    tools=[multiply]
)
agent.invoke({"messages": [{"role": "user", "content": "what's 42 x 7?"}]})
```

:::

:::js
To create a tool-calling agent, you can use the prebuilt [createReactAgent](https://js.langchain.com/docs/api/langgraph_prebuilt/functions/createReactAgent.html):

```typescript
import { tool } from "@langchain/core/tools";
import { z } from "zod";
// highlight-next-line
import { createReactAgent } from "@langchain/langgraph/prebuilt";

const multiply = tool(
  (input) => {
    return input.a * input.b;
  },
  {
    name: "multiply",
    description: "Multiply two numbers.",
    schema: z.object({
      a: z.number().describe("First operand"),
      b: z.number().describe("Second operand"),
    }),
  }
);

// highlight-next-line
const agent = createReactAgent({
  llm: new ChatAnthropic({ model: "claude-3-5-sonnet-20240620" }),
  tools: [multiply],
});

await agent.invoke({
  messages: [{ role: "user", content: "what's 42 x 7?" }],
});
```

:::

:::python

### Dynamically select tools

Configure tool availability at runtime based on context:

```python
from dataclasses import dataclass
from typing import Literal

from langchain.chat_models import init_chat_model
from langchain_core.tools import tool

from langgraph.prebuilt import create_react_agent
from langgraph.prebuilt.chat_agent_executor import AgentState
from langgraph.runtime import Runtime


@dataclass
class CustomContext:
    tools: list[Literal["weather", "compass"]]


@tool
def weather() -> str:
    """Returns the current weather conditions."""
    return "It's nice and sunny."


@tool
def compass() -> str:
    """Returns the direction the user is facing."""
    return "North"

model = init_chat_model("anthropic:claude-sonnet-4-20250514")

# highlight-next-line
def configure_model(state: AgentState, runtime: Runtime[CustomContext]):
    """Configure the model with tools based on runtime context."""
    selected_tools = [
        tool
        for tool in [weather, compass]
        if tool.name in runtime.context.tools
    ]
    return model.bind_tools(selected_tools)


agent = create_react_agent(
    # Dynamically configure the model with tools based on runtime context
    # highlight-next-line
    configure_model,
    # Initialize with all tools available
    # highlight-next-line
    tools=[weather, compass]
)

output = agent.invoke(
    {
        "messages": [
            {
                "role": "user",
                "content": "Who are you and what tools do you have access to?",
            }
        ]
    },
    # highlight-next-line
    context=CustomContext(tools=["weather"]),  # Only enable the weather tool
)

print(output["messages"][-1].text())
```

!!! version-added "Added in version 0.6.0"

:::

## Use in a workflow

If you are writing a custom workflow, you will need to:

1. register the tools with the chat model
2. call the tool if the model decides to use it

:::python
Use `model.bind_tools()` to register the tools with the model.

```python
from langchain.chat_models import init_chat_model

model = init_chat_model(model="claude-3-5-haiku-latest")

# highlight-next-line
model_with_tools = model.bind_tools([multiply])
```

:::

:::js
Use `model.bindTools()` to register the tools with the model.

```typescript
import { ChatOpenAI } from "@langchain/openai";

const model = new ChatOpenAI({ model: "gpt-4o" });

// highlight-next-line
const modelWithTools = model.bindTools([multiply]);
```

:::

LLMs automatically determine if a tool invocation is necessary and handle calling the tool with the appropriate arguments.

??? example "Extended example: attach tools to a chat model"

    :::python
    ```python
    from langchain_core.tools import tool
    from langchain.chat_models import init_chat_model

    @tool
    def multiply(a: int, b: int) -> int:
        """Multiply two numbers."""
        return a * b

    model = init_chat_model(model="claude-3-5-haiku-latest")
    # highlight-next-line
    model_with_tools = model.bind_tools([multiply])

    response_message = model_with_tools.invoke("what's 42 x 7?")
    tool_call = response_message.tool_calls[0]

    multiply.invoke(tool_call)
    ```

    ```pycon
    ToolMessage(
        content='294',
        name='multiply',
        tool_call_id='toolu_0176DV4YKSD8FndkeuuLj36c'
    )
    ```
    :::

    :::js
    ```typescript
    import { tool } from "@langchain/core/tools";
    import { ChatOpenAI } from "@langchain/openai";
    import { z } from "zod";

    const multiply = tool(
      (input) => {
        return input.a * input.b;
      },
      {
        name: "multiply",
        description: "Multiply two numbers.",
        schema: z.object({
          a: z.number().describe("First operand"),
          b: z.number().describe("Second operand"),
        }),
      }
    );

    const model = new ChatOpenAI({ model: "gpt-4o" });
    // highlight-next-line
    const modelWithTools = model.bindTools([multiply]);

    const responseMessage = await modelWithTools.invoke("what's 42 x 7?");
    const toolCall = responseMessage.tool_calls[0];

    await multiply.invoke(toolCall);
    ```

    ```
    ToolMessage {
      content: "294",
      name: "multiply",
      tool_call_id: "toolu_0176DV4YKSD8FndkeuuLj36c"
    }
    ```
    :::

#### ToolNode

:::python
To execute tools in custom workflows, use the prebuilt @[`ToolNode`][ToolNode] or implement your own custom node.

`ToolNode` is a specialized node for executing tools in a workflow. It provides the following features:

- Supports both synchronous and asynchronous tools.
- Executes multiple tools concurrently.
- Handles errors during tool execution (`handle_tool_errors=True`, enabled by default). See [handling tool errors](#handle-errors) for more details.

`ToolNode` operates on [`MessagesState`](../concepts/low_level.md#messagesstate):

- **Input**: `MessagesState`, where the last message is an `AIMessage` containing the `tool_calls` parameter.
- **Output**: `MessagesState` updated with the resulting [`ToolMessage`](https://python.langchain.com/docs/concepts/messages/#toolmessage) from executed tools.

```python
# highlight-next-line
from langgraph.prebuilt import ToolNode

def get_weather(location: str):
    """Call to get the current weather."""
    if location.lower() in ["sf", "san francisco"]:
        return "It's 60 degrees and foggy."
    else:
        return "It's 90 degrees and sunny."

def get_coolest_cities():
    """Get a list of coolest cities"""
    return "nyc, sf"

# highlight-next-line
tool_node = ToolNode([get_weather, get_coolest_cities])
tool_node.invoke({"messages": [...]})
```

:::

:::js
To execute tools in custom workflows, use the prebuilt [`ToolNode`](https://js.langchain.com/docs/api/langgraph_prebuilt/classes/ToolNode.html) or implement your own custom node.

`ToolNode` is a specialized node for executing tools in a workflow. It provides the following features:

- Supports both synchronous and asynchronous tools.
- Executes multiple tools concurrently.
- Handles errors during tool execution (`handleToolErrors: true`, enabled by default). See [handling tool errors](#handle-errors) for more details.

- **Input**: `MessagesZodState`, where the last message is an `AIMessage` containing the `tool_calls` parameter.
- **Output**: `MessagesZodState` updated with the resulting [`ToolMessage`](https://js.langchain.com/docs/concepts/messages/#toolmessage) from executed tools.

```typescript
// highlight-next-line
import { ToolNode } from "@langchain/langgraph/prebuilt";

const getWeather = tool(
  (input) => {
    if (["sf", "san francisco"].includes(input.location.toLowerCase())) {
      return "It's 60 degrees and foggy.";
    } else {
      return "It's 90 degrees and sunny.";
    }
  },
  {
    name: "get_weather",
    description: "Call to get the current weather.",
    schema: z.object({
      location: z.string().describe("Location to get the weather for."),
    }),
  }
);

const getCoolestCities = tool(
  () => {
    return "nyc, sf";
  },
  {
    name: "get_coolest_cities",
    description: "Get a list of coolest cities",
    schema: z.object({
      noOp: z.string().optional().describe("No-op parameter."),
    }),
  }
);

// highlight-next-line
const toolNode = new ToolNode([getWeather, getCoolestCities]);
await toolNode.invoke({ messages: [...] });
```

:::

??? example "Single tool call"

    :::python
    ```python
    from langchain_core.messages import AIMessage
    from langgraph.prebuilt import ToolNode

    # Define tools
    @tool
    def get_weather(location: str):
        """Call to get the current weather."""
        if location.lower() in ["sf", "san francisco"]:
            return "It's 60 degrees and foggy."
        else:
            return "It's 90 degrees and sunny."

    # highlight-next-line
    tool_node = ToolNode([get_weather])

    message_with_single_tool_call = AIMessage(
        content="",
        tool_calls=[
            {
                "name": "get_weather",
                "args": {"location": "sf"},
                "id": "tool_call_id",
                "type": "tool_call",
            }
        ],
    )

    tool_node.invoke({"messages": [message_with_single_tool_call]})
    ```

    ```
    {'messages': [ToolMessage(content="It's 60 degrees and foggy.", name='get_weather', tool_call_id='tool_call_id')]}
    ```
    :::

    :::js
    ```typescript
    import { AIMessage } from "@langchain/core/messages";
    import { ToolNode } from "@langchain/langgraph/prebuilt";
    import { tool } from "@langchain/core/tools";
    import { z } from "zod";

    // Define tools
    const getWeather = tool(
      (input) => {
        if (["sf", "san francisco"].includes(input.location.toLowerCase())) {
          return "It's 60 degrees and foggy.";
        } else {
          return "It's 90 degrees and sunny.";
        }
      },
      {
        name: "get_weather",
        description: "Call to get the current weather.",
        schema: z.object({
          location: z.string().describe("Location to get the weather for."),
        }),
      }
    );

    // highlight-next-line
    const toolNode = new ToolNode([getWeather]);

    const messageWithSingleToolCall = new AIMessage({
      content: "",
      tool_calls: [
        {
          name: "get_weather",
          args: { location: "sf" },
          id: "tool_call_id",
          type: "tool_call",
        }
      ],
    });

    await toolNode.invoke({ messages: [messageWithSingleToolCall] });
    ```

    ```
    { messages: [ToolMessage { content: "It's 60 degrees and foggy.", name: "get_weather", tool_call_id: "tool_call_id" }] }
    ```
    :::

??? example "Multiple tool calls"

    :::python
    ```python
    from langchain_core.messages import AIMessage
    from langgraph.prebuilt import ToolNode

    # Define tools

    def get_weather(location: str):
        """Call to get the current weather."""
        if location.lower() in ["sf", "san francisco"]:
            return "It's 60 degrees and foggy."
        else:
            return "It's 90 degrees and sunny."

    def get_coolest_cities():
        """Get a list of coolest cities"""
        return "nyc, sf"

    # highlight-next-line
    tool_node = ToolNode([get_weather, get_coolest_cities])

    message_with_multiple_tool_calls = AIMessage(
        content="",
        tool_calls=[
            {
                "name": "get_coolest_cities",
                "args": {},
                "id": "tool_call_id_1",
                "type": "tool_call",
            },
            {
                "name": "get_weather",
                "args": {"location": "sf"},
                "id": "tool_call_id_2",
                "type": "tool_call",
            },
        ],
    )

    # highlight-next-line
    tool_node.invoke({"messages": [message_with_multiple_tool_calls]})  # (1)!
    ```

    1. `ToolNode` will execute both tools in parallel

    ```
    {
        'messages': [
            ToolMessage(content='nyc, sf', name='get_coolest_cities', tool_call_id='tool_call_id_1'),
            ToolMessage(content="It's 60 degrees and foggy.", name='get_weather', tool_call_id='tool_call_id_2')
        ]
    }
    ```
    :::

    :::js
    ```typescript
    import { AIMessage } from "@langchain/core/messages";
    import { ToolNode } from "@langchain/langgraph/prebuilt";
    import { tool } from "@langchain/core/tools";
    import { z } from "zod";

    // Define tools
    const getWeather = tool(
      (input) => {
        if (["sf", "san francisco"].includes(input.location.toLowerCase())) {
          return "It's 60 degrees and foggy.";
        } else {
          return "It's 90 degrees and sunny.";
        }
      },
      {
        name: "get_weather",
        description: "Call to get the current weather.",
        schema: z.object({
          location: z.string().describe("Location to get the weather for."),
        }),
      }
    );

    const getCoolestCities = tool(
      () => {
        return "nyc, sf";
      },
      {
        name: "get_coolest_cities",
        description: "Get a list of coolest cities",
        schema: z.object({
          noOp: z.string().optional().describe("No-op parameter."),
        }),
      }
    );

    // highlight-next-line
    const toolNode = new ToolNode([getWeather, getCoolestCities]);

    const messageWithMultipleToolCalls = new AIMessage({
      content: "",
      tool_calls: [
        {
          name: "get_coolest_cities",
          args: {},
          id: "tool_call_id_1",
          type: "tool_call",
        },
        {
          name: "get_weather",
          args: { location: "sf" },
          id: "tool_call_id_2",
          type: "tool_call",
        },
      ],
    });

    // highlight-next-line
    await toolNode.invoke({ messages: [messageWithMultipleToolCalls] }); // (1)!
    ```

    1. `ToolNode` will execute both tools in parallel

    ```
    {
      messages: [
        ToolMessage { content: "nyc, sf", name: "get_coolest_cities", tool_call_id: "tool_call_id_1" },
        ToolMessage { content: "It's 60 degrees and foggy.", name: "get_weather", tool_call_id: "tool_call_id_2" }
      ]
    }
    ```
    :::

??? example "Use with a chat model"

    :::python
    ```python
    from langchain.chat_models import init_chat_model
    from langgraph.prebuilt import ToolNode

    def get_weather(location: str):
        """Call to get the current weather."""
        if location.lower() in ["sf", "san francisco"]:
            return "It's 60 degrees and foggy."
        else:
            return "It's 90 degrees and sunny."

    # highlight-next-line
    tool_node = ToolNode([get_weather])

    model = init_chat_model(model="claude-3-5-haiku-latest")
    # highlight-next-line
    model_with_tools = model.bind_tools([get_weather])  # (1)!


    # highlight-next-line
    response_message = model_with_tools.invoke("what's the weather in sf?")
    tool_node.invoke({"messages": [response_message]})
    ```

    1. Use `.bind_tools()` to attach the tool schema to the chat model

    ```
    {'messages': [ToolMessage(content="It's 60 degrees and foggy.", name='get_weather', tool_call_id='toolu_01Pnkgw5JeTRxXAU7tyHT4UW')]}
    ```
    :::

    :::js
    ```typescript
    import { ChatOpenAI } from "@langchain/openai";
    import { ToolNode } from "@langchain/langgraph/prebuilt";
    import { tool } from "@langchain/core/tools";
    import { z } from "zod";

    const getWeather = tool(
      (input) => {
        if (["sf", "san francisco"].includes(input.location.toLowerCase())) {
          return "It's 60 degrees and foggy.";
        } else {
          return "It's 90 degrees and sunny.";
        }
      },
      {
        name: "get_weather",
        description: "Call to get the current weather.",
        schema: z.object({
          location: z.string().describe("Location to get the weather for."),
        }),
      }
    );

    // highlight-next-line
    const toolNode = new ToolNode([getWeather]);

    const model = new ChatOpenAI({ model: "gpt-4o" });
    // highlight-next-line
    const modelWithTools = model.bindTools([getWeather]); // (1)!

    // highlight-next-line
    const responseMessage = await modelWithTools.invoke("what's the weather in sf?");
    await toolNode.invoke({ messages: [responseMessage] });
    ```

    1. Use `.bindTools()` to attach the tool schema to the chat model

    ```
    { messages: [ToolMessage { content: "It's 60 degrees and foggy.", name: "get_weather", tool_call_id: "toolu_01Pnkgw5JeTRxXAU7tyHT4UW" }] }
    ```
    :::

??? example "Use in a tool-calling agent"

    This is an example of creating a tool-calling agent from scratch using `ToolNode`. You can also use LangGraph's prebuilt [agent](../agents/agents.md).

    :::python
    ```python
    from langchain.chat_models import init_chat_model
    from langgraph.prebuilt import ToolNode
    from langgraph.graph import StateGraph, MessagesState, START, END

    def get_weather(location: str):
        """Call to get the current weather."""
        if location.lower() in ["sf", "san francisco"]:
            return "It's 60 degrees and foggy."
        else:
            return "It's 90 degrees and sunny."

    # highlight-next-line
    tool_node = ToolNode([get_weather])

    model = init_chat_model(model="claude-3-5-haiku-latest")
    # highlight-next-line
    model_with_tools = model.bind_tools([get_weather])

    def should_continue(state: MessagesState):
        messages = state["messages"]
        last_message = messages[-1]
        if last_message.tool_calls:
            return "tools"
        return END

    def call_model(state: MessagesState):
        messages = state["messages"]
        response = model_with_tools.invoke(messages)
        return {"messages": [response]}

    builder = StateGraph(MessagesState)

    # Define the two nodes we will cycle between
    builder.add_node("call_model", call_model)
    # highlight-next-line
    builder.add_node("tools", tool_node)

    builder.add_edge(START, "call_model")
    builder.add_conditional_edges("call_model", should_continue, ["tools", END])
    builder.add_edge("tools", "call_model")

    graph = builder.compile()

    graph.invoke({"messages": [{"role": "user", "content": "what's the weather in sf?"}]})
    ```

    ```
    {
        'messages': [
            HumanMessage(content="what's the weather in sf?"),
            AIMessage(
                content=[{'text': "I'll help you check the weather in San Francisco right now.", 'type': 'text'}, {'id': 'toolu_01A4vwUEgBKxfFVc5H3v1CNs', 'input': {'location': 'San Francisco'}, 'name': 'get_weather', 'type': 'tool_use'}],
                tool_calls=[{'name': 'get_weather', 'args': {'location': 'San Francisco'}, 'id': 'toolu_01A4vwUEgBKxfFVc5H3v1CNs', 'type': 'tool_call'}]
            ),
            ToolMessage(content="It's 60 degrees and foggy."),
            AIMessage(content="The current weather in San Francisco is 60 degrees and foggy. Typical San Francisco weather with its famous marine layer!")
        ]
    }
    ```
    :::

    :::js
    ```typescript
    import { ChatOpenAI } from "@langchain/openai";
    import { ToolNode } from "@langchain/langgraph/prebuilt";
    import { StateGraph, MessagesZodState, START, END } from "@langchain/langgraph";
    import { tool } from "@langchain/core/tools";
    import { z } from "zod";
    import { isAIMessage } from "@langchain/core/messages";

    const getWeather = tool(
      (input) => {
        if (["sf", "san francisco"].includes(input.location.toLowerCase())) {
          return "It's 60 degrees and foggy.";
        } else {
          return "It's 90 degrees and sunny.";
        }
      },
      {
        name: "get_weather",
        description: "Call to get the current weather.",
        schema: z.object({
          location: z.string().describe("Location to get the weather for."),
        }),
      }
    );

    // highlight-next-line
    const toolNode = new ToolNode([getWeather]);

    const model = new ChatOpenAI({ model: "gpt-4o" });
    // highlight-next-line
    const modelWithTools = model.bindTools([getWeather]);

    const shouldContinue = (state: z.infer<typeof MessagesZodState>) => {
      const messages = state.messages;
      const lastMessage = messages.at(-1);
      if (lastMessage && isAIMessage(lastMessage) && lastMessage.tool_calls?.length) {
        return "tools";
      }
      return END;
    };

    const callModel = async (state: z.infer<typeof MessagesZodState>) => {
      const messages = state.messages;
      const response = await modelWithTools.invoke(messages);
      return { messages: [response] };
    };

    const builder = new StateGraph(MessagesZodState)
      // Define the two nodes we will cycle between
      .addNode("agent", callModel)
      // highlight-next-line
      .addNode("tools", toolNode)
      .addEdge(START, "agent")
      .addConditionalEdges("agent", shouldContinue, ["tools", END])
      .addEdge("tools", "agent");

    const graph = builder.compile();

    await graph.invoke({
      messages: [{ role: "user", content: "what's the weather in sf?" }]
    });
    ```

    ```
    {
      messages: [
        HumanMessage { content: "what's the weather in sf?" },
        AIMessage {
          content: [{ text: "I'll help you check the weather in San Francisco right now.", type: "text" }, { id: "toolu_01A4vwUEgBKxfFVc5H3v1CNs", input: { location: "San Francisco" }, name: "get_weather", type: "tool_use" }],
          tool_calls: [{ name: "get_weather", args: { location: "San Francisco" }, id: "toolu_01A4vwUEgBKxfFVc5H3v1CNs", type: "tool_call" }]
        },
        ToolMessage { content: "It's 60 degrees and foggy." },
        AIMessage { content: "The current weather in San Francisco is 60 degrees and foggy. Typical San Francisco weather with its famous marine layer!" }
      ]
    }
    ```
    :::

## Tool customization

For more control over tool behavior, use the `@tool` decorator.

### Parameter descriptions

:::python
Auto-generate descriptions from docstrings:

```python
# highlight-next-line
from langchain_core.tools import tool

# highlight-next-line
@tool("multiply_tool", parse_docstring=True)
def multiply(a: int, b: int) -> int:
    """Multiply two numbers.

    Args:
        a: First operand
        b: Second operand
    """
    return a * b
```

:::

:::js
Auto-generate descriptions from schema:

```typescript
import { tool } from "@langchain/core/tools";
import { z } from "zod";

// highlight-next-line
const multiply = tool(
  (input) => {
    return input.a * input.b;
  },
  {
    name: "multiply",
    description: "Multiply two numbers.",
    schema: z.object({
      a: z.number().describe("First operand"),
      b: z.number().describe("Second operand"),
    }),
  }
);
```

:::

### Explicit input schema

:::python
Define schemas using `args_schema`:

```python
from pydantic import BaseModel, Field
from langchain_core.tools import tool

class MultiplyInputSchema(BaseModel):
    """Multiply two numbers"""
    a: int = Field(description="First operand")
    b: int = Field(description="Second operand")

# highlight-next-line
@tool("multiply_tool", args_schema=MultiplyInputSchema)
def multiply(a: int, b: int) -> int:
    return a * b
```

:::

### Tool name

Override the default tool name using the first argument or name property:

:::python

```python
from langchain_core.tools import tool

# highlight-next-line
@tool("multiply_tool")
def multiply(a: int, b: int) -> int:
    """Multiply two numbers."""
    return a * b
```

:::

:::js

```typescript
import { tool } from "@langchain/core/tools";
import { z } from "zod";

// highlight-next-line
const multiply = tool(
  (input) => {
    return input.a * input.b;
  },
  {
    name: "multiply_tool", // Custom name
    description: "Multiply two numbers.",
    schema: z.object({
      a: z.number().describe("First operand"),
      b: z.number().describe("Second operand"),
    }),
  }
);
```

:::

## Context management

Tools within LangGraph sometimes require context data, such as runtime-only arguments (e.g., user IDs or session details), that should not be controlled by the model. LangGraph provides three methods for managing such context:

| Type                                    | Usage Scenario                           | Mutable | Lifetime                 |
| --------------------------------------- | ---------------------------------------- | ------- | ------------------------ |
| [Configuration](#configuration)         | Static, immutable runtime data           |       | Single invocation        |
| [Short-term memory](#short-term-memory) | Dynamic, changing data during invocation |       | Single invocation        |
| [Long-term memory](#long-term-memory)   | Persistent, cross-session data           |       | Across multiple sessions |

### Configuration

:::python
Use configuration when you have **immutable** runtime data that tools require, such as user identifiers. You pass these arguments via [`RunnableConfig`](https://python.langchain.com/docs/concepts/runnables/#runnableconfig) at invocation and access them in the tool:

```python
from langchain_core.tools import tool
from langchain_core.runnables import RunnableConfig

@tool
# highlight-next-line
def get_user_info(config: RunnableConfig) -> str:
    """Retrieve user information based on user ID."""
    user_id = config["configurable"].get("user_id")
    return "User is John Smith" if user_id == "user_123" else "Unknown user"

# Invocation example with an agent
agent.invoke(
    {"messages": [{"role": "user", "content": "look up user info"}]},
    # highlight-next-line
    config={"configurable": {"user_id": "user_123"}}
)
```

:::

:::js
Use configuration when you have **immutable** runtime data that tools require, such as user identifiers. You pass these arguments via [`LangGraphRunnableConfig`](https://js.langchain.com/docs/api/langgraph/interfaces/LangGraphRunnableConfig.html) at invocation and access them in the tool:

```typescript
import { tool } from "@langchain/core/tools";
import { z } from "zod";
import type { LangGraphRunnableConfig } from "@langchain/langgraph";

const getUserInfo = tool(
  // highlight-next-line
  async (_, config: LangGraphRunnableConfig) => {
    const userId = config?.configurable?.user_id;
    return userId === "user_123" ? "User is John Smith" : "Unknown user";
  },
  {
    name: "get_user_info",
    description: "Retrieve user information based on user ID.",
    schema: z.object({}),
  }
);

// Invocation example with an agent
await agent.invoke(
  { messages: [{ role: "user", content: "look up user info" }] },
  // highlight-next-line
  { configurable: { user_id: "user_123" } }
);
```

:::

??? example "Extended example: Access config in tools"

    :::python
    ```python
    from langchain_core.runnables import RunnableConfig
    from langchain_core.tools import tool
    from langgraph.prebuilt import create_react_agent

    def get_user_info(
        # highlight-next-line
        config: RunnableConfig,
    ) -> str:
        """Look up user info."""
        # highlight-next-line
        user_id = config["configurable"].get("user_id")
        return "User is John Smith" if user_id == "user_123" else "Unknown user"

    agent = create_react_agent(
        model="anthropic:claude-3-7-sonnet-latest",
        tools=[get_user_info],
    )

    agent.invoke(
        {"messages": [{"role": "user", "content": "look up user information"}]},
        # highlight-next-line
        config={"configurable": {"user_id": "user_123"}}
    )
    ```
    :::

    :::js
    ```typescript
    import { tool } from "@langchain/core/tools";
    import { z } from "zod";
    import { createReactAgent } from "@langchain/langgraph/prebuilt";
    import type { LangGraphRunnableConfig } from "@langchain/langgraph";
    import { ChatAnthropic } from "@langchain/anthropic";

    const getUserInfo = tool(
      // highlight-next-line
      async (_, config: LangGraphRunnableConfig) => {
        // highlight-next-line
        const userId = config?.configurable?.user_id;
        return userId === "user_123" ? "User is John Smith" : "Unknown user";
      },
      {
        name: "get_user_info",
        description: "Look up user info.",
        schema: z.object({}),
      }
    );

    const agent = createReactAgent({
      llm: new ChatAnthropic({ model: "claude-3-5-sonnet-20240620" }),
      tools: [getUserInfo],
    });

    await agent.invoke(
      { messages: [{ role: "user", content: "look up user information" }] },
      // highlight-next-line
      { configurable: { user_id: "user_123" } }
    );
    ```
    :::

### Short-term memory

Short-term memory maintains **dynamic** state that changes during a single execution.

:::python
To **access** (read) the graph state inside the tools, you can use a special parameter **annotation**  @[`InjectedState`][InjectedState]:

```python
from typing import Annotated, NotRequired
from langchain_core.tools import tool
from langgraph.prebuilt import InjectedState, create_react_agent
from langgraph.prebuilt.chat_agent_executor import AgentState

class CustomState(AgentState):
    # The user_name field in short-term state
    user_name: NotRequired[str]

@tool
def get_user_name(
    # highlight-next-line
    state: Annotated[CustomState, InjectedState]
) -> str:
    """Retrieve the current user-name from state."""
    # Return stored name or a default if not set
    return state.get("user_name", "Unknown user")

# Example agent setup
agent = create_react_agent(
    model="anthropic:claude-3-7-sonnet-latest",
    tools=[get_user_name],
    state_schema=CustomState,
)

# Invocation: reads the name from state (initially empty)
agent.invoke({"messages": "what's my name?"})
```

:::

:::js
To **access** (read) the graph state inside the tools, you can use the @[`getContextVariable`][getContextVariable] function:

```typescript
import { tool } from "@langchain/core/tools";
import { z } from "zod";
import { getContextVariable } from "@langchain/core/context";
import { MessagesZodState } from "@langchain/langgraph";
import type { LangGraphRunnableConfig } from "@langchain/langgraph";

const getUserName = tool(
  // highlight-next-line
  async (_, config: LangGraphRunnableConfig) => {
    // highlight-next-line
    const currentState = getContextVariable("currentState") as z.infer<
      typeof MessagesZodState
    > & { userName?: string };
    return currentState?.userName || "Unknown user";
  },
  {
    name: "get_user_name",
    description: "Retrieve the current user name from state.",
    schema: z.object({}),
  }
);
```

:::

:::python
Use a tool that returns a `Command` to **update** `user_name` and append a confirmation message:

```python
from typing import Annotated
from langgraph.types import Command
from langchain_core.messages import ToolMessage
from langchain_core.tools import tool, InjectedToolCallId

@tool
def update_user_name(
    new_name: str,
    tool_call_id: Annotated[str, InjectedToolCallId]
) -> Command:
    """Update user-name in short-term memory."""
    # highlight-next-line
    return Command(update={
        # highlight-next-line
        "user_name": new_name,
        # highlight-next-line
        "messages": [
            # highlight-next-line
            ToolMessage(f"Updated user name to {new_name}", tool_call_id=tool_call_id)
            # highlight-next-line
        ]
        # highlight-next-line
    })
```

:::

:::js
To **update** short-term memory, you can use tools that return a `Command` to update state:

```typescript
import { Command } from "@langchain/langgraph";
import { tool } from "@langchain/core/tools";
import { z } from "zod";

const updateUserName = tool(
  async (input) => {
    // highlight-next-line
    return new Command({
      // highlight-next-line
      update: {
        // highlight-next-line
        userName: input.newName,
        // highlight-next-line
        messages: [
          // highlight-next-line
          {
            // highlight-next-line
            role: "assistant",
            // highlight-next-line
            content: `Updated user name to ${input.newName}`,
            // highlight-next-line
          },
          // highlight-next-line
        ],
        // highlight-next-line
      },
      // highlight-next-line
    });
  },
  {
    name: "update_user_name",
    description: "Update user name in short-term memory.",
    schema: z.object({
      newName: z.string().describe("The new user name"),
    }),
  }
);
```

:::

!!! important

    :::python
    If you want to use tools that return `Command` and update graph state, you can either use prebuilt @[`create_react_agent`][create_react_agent] / @[`ToolNode`][ToolNode] components, or implement your own tool-executing node that collects `Command` objects returned by the tools and returns a list of them, e.g.:

    ```python
    def call_tools(state):
        ...
        commands = [tools_by_name[tool_call["name"]].invoke(tool_call) for tool_call in tool_calls]
        return commands
    ```
    :::

    :::js
    If you want to use tools that return `Command` and update graph state, you can either use prebuilt @[`createReactAgent`][create_react_agent] / @[ToolNode] components, or implement your own tool-executing node that collects `Command` objects returned by the tools and returns a list of them, e.g.:

    ```typescript
    const callTools = async (state: State) => {
      // ...
      const commands = await Promise.all(
        toolCalls.map(toolCall => toolsByName[toolCall.name].invoke(toolCall))
      );
      return commands;
    };
    ```
    :::

### Long-term memory

Use [long-term memory](../concepts/memory.md#long-term-memory) to store user-specific or application-specific data across conversations. This is useful for applications like chatbots, where you want to remember user preferences or other information.

To use long-term memory, you need to:

1. [Configure a store](memory/add-memory.md#add-long-term-memory) to persist data across invocations.
2. Access the store from within tools.

:::python
To **access** information in the store:

```python
from langchain_core.runnables import RunnableConfig
from langchain_core.tools import tool
from langgraph.graph import StateGraph
# highlight-next-line
from langgraph.config import get_store

@tool
def get_user_info(config: RunnableConfig) -> str:
    """Look up user info."""
    # Same as that provided to `builder.compile(store=store)`
    # or `create_react_agent`
    # highlight-next-line
    store = get_store()
    user_id = config["configurable"].get("user_id")
    # highlight-next-line
    user_info = store.get(("users",), user_id)
    return str(user_info.value) if user_info else "Unknown user"

builder = StateGraph(...)
...
graph = builder.compile(store=store)
```

:::

:::js
To **access** information in the store:

```typescript
import { tool } from "@langchain/core/tools";
import { z } from "zod";
import type { LangGraphRunnableConfig } from "@langchain/langgraph";

const getUserInfo = tool(
  async (_, config: LangGraphRunnableConfig) => {
    // Same as that provided to `builder.compile({ store })`
    // or `createReactAgent`
    // highlight-next-line
    const store = config.store;
    if (!store) throw new Error("Store not provided");

    const userId = config?.configurable?.user_id;
    // highlight-next-line
    const userInfo = await store.get(["users"], userId);
    return userInfo?.value ? JSON.stringify(userInfo.value) : "Unknown user";
  },
  {
    name: "get_user_info",
    description: "Look up user info.",
    schema: z.object({}),
  }
);
```

:::

??? example "Access long-term memory"

    :::python
    ```python
    from langchain_core.runnables import RunnableConfig
    from langchain_core.tools import tool
    from langgraph.config import get_store
    from langgraph.prebuilt import create_react_agent
    from langgraph.store.memory import InMemoryStore

    # highlight-next-line
    store = InMemoryStore() # (1)!

    # highlight-next-line
    store.put(  # (2)!
        ("users",),  # (3)!
        "user_123",  # (4)!
        {
            "name": "John Smith",
            "language": "English",
        } # (5)!
    )

    @tool
    def get_user_info(config: RunnableConfig) -> str:
        """Look up user info."""
        # Same as that provided to `create_react_agent`
        # highlight-next-line
        store = get_store() # (6)!
        user_id = config["configurable"].get("user_id")
        # highlight-next-line
        user_info = store.get(("users",), user_id) # (7)!
        return str(user_info.value) if user_info else "Unknown user"

    agent = create_react_agent(
        model="anthropic:claude-3-7-sonnet-latest",
        tools=[get_user_info],
        # highlight-next-line
        store=store # (8)!
    )

    # Run the agent
    agent.invoke(
        {"messages": [{"role": "user", "content": "look up user information"}]},
        # highlight-next-line
        config={"configurable": {"user_id": "user_123"}}
    )
    ```

    1. The `InMemoryStore` is a store that stores data in memory. In a production setting, you would typically use a database or other persistent storage. Please review the [store documentation][../reference/store.md) for more options. If you're deploying with **LangGraph Platform**, the platform will provide a production-ready store for you.
    2. For this example, we write some sample data to the store using the `put` method. Please see the @[BaseStore.put] API reference for more details.
    3. The first argument is the namespace. This is used to group related data together. In this case, we are using the `users` namespace to group user data.
    4. A key within the namespace. This example uses a user ID for the key.
    5. The data that we want to store for the given user.
    6. The `get_store` function is used to access the store. You can call it from anywhere in your code, including tools and prompts. This function returns the store that was passed to the agent when it was created.
    7. The `get` method is used to retrieve data from the store. The first argument is the namespace, and the second argument is the key. This will return a `StoreValue` object, which contains the value and metadata about the value.
    8. The `store` is passed to the agent. This enables the agent to access the store when running tools. You can also use the `get_store` function to access the store from anywhere in your code.
    :::

    :::js
    ```typescript
    import { tool } from "@langchain/core/tools";
    import { z } from "zod";
    import { createReactAgent } from "@langchain/langgraph/prebuilt";
    import { InMemoryStore } from "@langchain/langgraph";
    import { ChatAnthropic } from "@langchain/anthropic";
    import type { LangGraphRunnableConfig } from "@langchain/langgraph";

    // highlight-next-line
    const store = new InMemoryStore(); // (1)!

    // highlight-next-line
    await store.put(  // (2)!
      ["users"],  // (3)!
      "user_123",  // (4)!
      {
        name: "John Smith",
        language: "English",
      } // (5)!
    );

    const getUserInfo = tool(
      async (_, config: LangGraphRunnableConfig) => {
        // Same as that provided to `createReactAgent`
        // highlight-next-line
        const store = config.store; // (6)!
        if (!store) throw new Error("Store not provided");

        const userId = config?.configurable?.user_id;
        // highlight-next-line
        const userInfo = await store.get(["users"], userId); // (7)!
        return userInfo?.value ? JSON.stringify(userInfo.value) : "Unknown user";
      },
      {
        name: "get_user_info",
        description: "Look up user info.",
        schema: z.object({}),
      }
    );

    const agent = createReactAgent({
      llm: new ChatAnthropic({ model: "claude-3-5-sonnet-20240620" }),
      tools: [getUserInfo],
      // highlight-next-line
      store: store // (8)!
    });

    // Run the agent
    await agent.invoke(
      { messages: [{ role: "user", content: "look up user information" }] },
      // highlight-next-line
      { configurable: { user_id: "user_123" } }
    );
    ```

    1. The `InMemoryStore` is a store that stores data in memory. In production, you would typically use a database or other persistent storage. Please review the [store documentation](../reference/store.md) for more options. If you're deploying with **LangGraph Platform**, the platform will provide a production-ready store for you.
    2. For this example, we write some sample data to the store using the `put` method. Please see the [BaseStore.put](https://js.langchain.com/docs/api/langgraph_store/classes/BaseStore.html#put) API reference for more details.
    3. The first argument is the namespace. This is used to group related data together. In this case, we are using the `users` namespace to group user data.
    4. A key within the namespace. This example uses a user ID for the key.
    5. The data that we want to store for the given user.
    6. The store is accessible from the config object that is passed to the tool. This enables the tool to access the store when running.
    7. The `get` method is used to retrieve data from the store. The first argument is the namespace, and the second argument is the key. This will return a `StoreValue` object, which contains the value and metadata about the value.
    8. The `store` is passed to the agent. This enables the agent to access the store when running tools.
    :::

:::python
To **update** information in the store:

```python
from langchain_core.runnables import RunnableConfig
from langchain_core.tools import tool
from langgraph.graph import StateGraph
# highlight-next-line
from langgraph.config import get_store

@tool
def save_user_info(user_info: str, config: RunnableConfig) -> str:
    """Save user info."""
    # Same as that provided to `builder.compile(store=store)`
    # or `create_react_agent`
    # highlight-next-line
    store = get_store()
    user_id = config["configurable"].get("user_id")
    # highlight-next-line
    store.put(("users",), user_id, user_info)
    return "Successfully saved user info."

builder = StateGraph(...)
...
graph = builder.compile(store=store)
```

:::

:::js
To **update** information in the store:

```typescript
import { tool } from "@langchain/core/tools";
import { z } from "zod";
import type { LangGraphRunnableConfig } from "@langchain/langgraph";

const saveUserInfo = tool(
  async (input, config: LangGraphRunnableConfig) => {
    // Same as that provided to `builder.compile({ store })`
    // or `createReactAgent`
    // highlight-next-line
    const store = config.store;
    if (!store) throw new Error("Store not provided");

    const userId = config?.configurable?.user_id;
    // highlight-next-line
    await store.put(["users"], userId, input.userInfo);
    return "Successfully saved user info.";
  },
  {
    name: "save_user_info",
    description: "Save user info.",
    schema: z.object({
      userInfo: z.string().describe("User information to save"),
    }),
  }
);
```

:::

??? example "Update long-term memory"

    :::python
    ```python
    from typing_extensions import TypedDict

    from langchain_core.tools import tool
    from langgraph.config import get_store
    from langchain_core.runnables import RunnableConfig
    from langgraph.prebuilt import create_react_agent
    from langgraph.store.memory import InMemoryStore

    store = InMemoryStore() # (1)!

    class UserInfo(TypedDict): # (2)!
        name: str

    @tool
    def save_user_info(user_info: UserInfo, config: RunnableConfig) -> str: # (3)!
        """Save user info."""
        # Same as that provided to `create_react_agent`
        # highlight-next-line
        store = get_store() # (4)!
        user_id = config["configurable"].get("user_id")
        # highlight-next-line
        store.put(("users",), user_id, user_info) # (5)!
        return "Successfully saved user info."

    agent = create_react_agent(
        model="anthropic:claude-3-7-sonnet-latest",
        tools=[save_user_info],
        # highlight-next-line
        store=store
    )

    # Run the agent
    agent.invoke(
        {"messages": [{"role": "user", "content": "My name is John Smith"}]},
        # highlight-next-line
        config={"configurable": {"user_id": "user_123"}} # (6)!
    )

    # You can access the store directly to get the value
    store.get(("users",), "user_123").value
    ```

    1. The `InMemoryStore` is a store that stores data in memory. In a production setting, you would typically use a database or other persistent storage. Please review the [store documentation](../reference/store.md) for more options. If you're deploying with **LangGraph Platform**, the platform will provide a production-ready store for you.
    2. The `UserInfo` class is a `TypedDict` that defines the structure of the user information. The LLM will use this to format the response according to the schema.
    3. The `save_user_info` function is a tool that allows an agent to update user information. This could be useful for a chat application where the user wants to update their profile information.
    4. The `get_store` function is used to access the store. You can call it from anywhere in your code, including tools and prompts. This function returns the store that was passed to the agent when it was created.
    5. The `put` method is used to store data in the store. The first argument is the namespace, and the second argument is the key. This will store the user information in the store.
    6. The `user_id` is passed in the config. This is used to identify the user whose information is being updated.
    :::

    :::js
    ```typescript
    import { tool } from "@langchain/core/tools";
    import { z } from "zod";
    import { createReactAgent } from "@langchain/langgraph/prebuilt";
    import { InMemoryStore } from "@langchain/langgraph";
    import { ChatAnthropic } from "@langchain/anthropic";
    import type { LangGraphRunnableConfig } from "@langchain/langgraph";

    const store = new InMemoryStore(); // (1)!

    const UserInfoSchema = z.object({ // (2)!
      name: z.string(),
    });

    const saveUserInfo = tool(
      async (input, config: LangGraphRunnableConfig) => { // (3)!
        // Same as that provided to `createReactAgent`
        // highlight-next-line
        const store = config.store; // (4)!
        if (!store) throw new Error("Store not provided");

        const userId = config?.configurable?.user_id;
        // highlight-next-line
        await store.put(["users"], userId, input); // (5)!
        return "Successfully saved user info.";
      },
      {
        name: "save_user_info",
        description: "Save user info.",
        schema: UserInfoSchema,
      }
    );

    const agent = createReactAgent({
      llm: new ChatAnthropic({ model: "claude-3-5-sonnet-20240620" }),
      tools: [saveUserInfo],
      // highlight-next-line
      store: store
    });

    // Run the agent
    await agent.invoke(
      { messages: [{ role: "user", content: "My name is John Smith" }] },
      // highlight-next-line
      { configurable: { user_id: "user_123" } } // (6)!
    );

    // You can access the store directly to get the value
    const userInfo = await store.get(["users"], "user_123");
    console.log(userInfo?.value);
    ```

    1. The `InMemoryStore` is a store that stores data in memory. In production, you would typically use a database or other persistent storage. Please review the [store documentation](../reference/store.md) for more options. If you're deploying with **LangGraph Platform**, the platform will provide a production-ready store for you.
    2. The `UserInfoSchema` is a Zod schema that defines the structure of the user information. The LLM will use this to format the response according to the schema.
    3. The `saveUserInfo` function is a tool that allows an agent to update user information. This could be useful for a chat application where the user wants to update their profile information.
    4. The store is accessible from the config object that is passed to the tool. This enables the tool to access the store when running.
    5. The `put` method is used to store data in the store. The first argument is the namespace, and the second argument is the key. This will store the user information in the store.
    6. The `user_id` is passed in the config. This is used to identify the user whose information is being updated.
    :::

## Advanced tool features

### Immediate return

:::python
Use `return_direct=True` to immediately return a tool's result without executing additional logic.

This is useful for tools that should not trigger further processing or tool calls, allowing you to return results directly to the user.

```python
# highlight-next-line
@tool(return_direct=True)
def add(a: int, b: int) -> int:
    """Add two numbers"""
    return a + b
```

:::

:::js
Use `returnDirect: true` to immediately return a tool's result without executing additional logic.

This is useful for tools that should not trigger further processing or tool calls, allowing you to return results directly to the user.

```typescript
import { tool } from "@langchain/core/tools";
import { z } from "zod";

// highlight-next-line
const add = tool(
  (input) => {
    return input.a + input.b;
  },
  {
    name: "add",
    description: "Add two numbers",
    schema: z.object({
      a: z.number(),
      b: z.number(),
    }),
    // highlight-next-line
    returnDirect: true,
  }
);
```

:::

??? example "Extended example: Using return_direct in a prebuilt agent"

    :::python
    ```python
    from langchain_core.tools import tool
    from langgraph.prebuilt import create_react_agent

    # highlight-next-line
    @tool(return_direct=True)
    def add(a: int, b: int) -> int:
        """Add two numbers"""
        return a + b

    agent = create_react_agent(
        model="anthropic:claude-3-7-sonnet-latest",
        tools=[add]
    )

    agent.invoke(
        {"messages": [{"role": "user", "content": "what's 3 + 5?"}]}
    )
    ```
    :::

    :::js
    ```typescript
    import { tool } from "@langchain/core/tools";
    import { z } from "zod";
    import { createReactAgent } from "@langchain/langgraph/prebuilt";
    import { ChatAnthropic } from "@langchain/anthropic";

    // highlight-next-line
    const add = tool(
      (input) => {
        return input.a + input.b;
      },
      {
        name: "add",
        description: "Add two numbers",
        schema: z.object({
          a: z.number(),
          b: z.number(),
        }),
        // highlight-next-line
        returnDirect: true,
      }
    );

    const agent = createReactAgent({
      llm: new ChatAnthropic({ model: "claude-3-5-sonnet-20240620" }),
      tools: [add]
    });

    await agent.invoke({
      messages: [{ role: "user", content: "what's 3 + 5?" }]
    });
    ```
    :::

!!! important "Using without prebuilt components"

    :::python
    If you are building a custom workflow and are not relying on `create_react_agent` or `ToolNode`, you will also
    need to implement the control flow to handle `return_direct=True`.
    :::

    :::js
    If you are building a custom workflow and are not relying on `createReactAgent` or `ToolNode`, you will also
    need to implement the control flow to handle `returnDirect: true`.
    :::

### Force tool use

If you need to force a specific tool to be used, you will need to configure this at the **model** level using the `tool_choice` parameter in the bind_tools method.

Force specific tool usage via tool_choice:

:::python

```python
@tool(return_direct=True)
def greet(user_name: str) -> int:
    """Greet user."""
    return f"Hello {user_name}!"

tools = [greet]

configured_model = model.bind_tools(
    tools,
    # Force the use of the 'greet' tool
    # highlight-next-line
    tool_choice={"type": "tool", "name": "greet"}
)
```

:::

:::js

```typescript
const greet = tool(
  (input) => {
    return `Hello ${input.userName}!`;
  },
  {
    name: "greet",
    description: "Greet user.",
    schema: z.object({
      userName: z.string(),
    }),
    returnDirect: true,
  }
);

const tools = [greet];

const configuredModel = model.bindTools(
  tools,
  // Force the use of the 'greet' tool
  // highlight-next-line
  { tool_choice: { type: "tool", name: "greet" } }
);
```

:::

??? example "Extended example: Force tool usage in an agent"

    :::python
    To force the agent to use specific tools, you can set the `tool_choice` option in `model.bind_tools()`:

    ```python
    from langchain_core.tools import tool

    # highlight-next-line
    @tool(return_direct=True)
    def greet(user_name: str) -> int:
        """Greet user."""
        return f"Hello {user_name}!"

    tools = [greet]

    agent = create_react_agent(
        # highlight-next-line
        model=model.bind_tools(tools, tool_choice={"type": "tool", "name": "greet"}),
        tools=tools
    )

    agent.invoke(
        {"messages": [{"role": "user", "content": "Hi, I am Bob"}]}
    )
    ```
    :::

    :::js
    To force the agent to use specific tools, you can set the `tool_choice` option in `model.bindTools()`:

    ```typescript
    import { tool } from "@langchain/core/tools";
    import { z } from "zod";
    import { createReactAgent } from "@langchain/langgraph/prebuilt";
    import { ChatOpenAI } from "@langchain/openai";

    // highlight-next-line
    const greet = tool(
      (input) => {
        return `Hello ${input.userName}!`;
      },
      {
        name: "greet",
        description: "Greet user.",
        schema: z.object({
          userName: z.string(),
        }),
        // highlight-next-line
        returnDirect: true,
      }
    );

    const tools = [greet];
    const model = new ChatOpenAI({ model: "gpt-4o" });

    const agent = createReactAgent({
      // highlight-next-line
      llm: model.bindTools(tools, { tool_choice: { type: "tool", name: "greet" } }),
      tools: tools
    });

    await agent.invoke({
      messages: [{ role: "user", content: "Hi, I am Bob" }]
    });
    ```
    :::

!!! Warning "Avoid infinite loops"

    :::python
    Forcing tool usage without stopping conditions can create infinite loops. Use one of the following safeguards:

    - Mark the tool with [`return_direct=True`](#immediate-return) to end the loop after execution.
    - Set [`recursion_limit`](../concepts/low_level.md#recursion-limit) to restrict the number of execution steps.
    :::

    :::js
    Forcing tool usage without stopping conditions can create infinite loops. Use one of the following safeguards:

    - Mark the tool with [`returnDirect: true`](#immediate-return) to end the loop after execution.
    - Set [`recursionLimit`](../concepts/low_level.md#recursion-limit) to restrict the number of execution steps.
    :::

!!! tip "Tool choice configuration"

    The `tool_choice` parameter is used to configure which tool should be used by the model when it decides to call a tool. This is useful when you want to ensure that a specific tool is always called for a particular task or when you want to override the model's default behavior of choosing a tool based on its internal logic.

    Note that not all models support this feature, and the exact configuration may vary depending on the model you are using.

### Disable parallel calls

:::python
For supported providers, you can disable parallel tool calling by setting `parallel_tool_calls=False` via the `model.bind_tools()` method:

```python
model.bind_tools(
    tools,
    # highlight-next-line
    parallel_tool_calls=False
)
```

:::

:::js
For supported providers, you can disable parallel tool calling by setting `parallel_tool_calls: false` via the `model.bindTools()` method:

```typescript
model.bindTools(
  tools,
  // highlight-next-line
  { parallel_tool_calls: false }
);
```

:::

??? example "Extended example: disable parallel tool calls in a prebuilt agent"

    :::python
    ```python
    from langchain.chat_models import init_chat_model

    def add(a: int, b: int) -> int:
        """Add two numbers"""
        return a + b

    def multiply(a: int, b: int) -> int:
        """Multiply two numbers."""
        return a * b

    model = init_chat_model("anthropic:claude-3-5-sonnet-latest", temperature=0)
    tools = [add, multiply]
    agent = create_react_agent(
        # disable parallel tool calls
        # highlight-next-line
        model=model.bind_tools(tools, parallel_tool_calls=False),
        tools=tools
    )

    agent.invoke(
        {"messages": [{"role": "user", "content": "what's 3 + 5 and 4 * 7?"}]}
    )
    ```
    :::

    :::js
    ```typescript
    import { ChatOpenAI } from "@langchain/openai";
    import { tool } from "@langchain/core/tools";
    import { z } from "zod";
    import { createReactAgent } from "@langchain/langgraph/prebuilt";

    const add = tool(
      (input) => {
        return input.a + input.b;
      },
      {
        name: "add",
        description: "Add two numbers",
        schema: z.object({
          a: z.number(),
          b: z.number(),
        }),
      }
    );

    const multiply = tool(
      (input) => {
        return input.a * input.b;
      },
      {
        name: "multiply",
        description: "Multiply two numbers.",
        schema: z.object({
          a: z.number(),
          b: z.number(),
        }),
      }
    );

    const model = new ChatOpenAI({ model: "gpt-4o", temperature: 0 });
    const tools = [add, multiply];

    const agent = createReactAgent({
      // disable parallel tool calls
      // highlight-next-line
      llm: model.bindTools(tools, { parallel_tool_calls: false }),
      tools: tools
    });

    await agent.invoke({
      messages: [{ role: "user", content: "what's 3 + 5 and 4 * 7?" }]
    });
    ```
    :::

### Handle errors

:::python
LangGraph provides built-in error handling for tool execution through the prebuilt @[ToolNode][ToolNode] component, used both independently and in prebuilt agents.

By **default**, `ToolNode` catches exceptions raised during tool execution and returns them as `ToolMessage` objects with a status indicating an error.

```python
from langchain_core.messages import AIMessage
from langgraph.prebuilt import ToolNode

def multiply(a: int, b: int) -> int:
    if a == 42:
        raise ValueError("The ultimate error")
    return a * b

# Default error handling (enabled by default)
tool_node = ToolNode([multiply])

message = AIMessage(
    content="",
    tool_calls=[{
        "name": "multiply",
        "args": {"a": 42, "b": 7},
        "id": "tool_call_id",
        "type": "tool_call"
    }]
)

result = tool_node.invoke({"messages": [message]})
```

Output:

```pycon
{'messages': [
    ToolMessage(
        content="Error: ValueError('The ultimate error')\n Please fix your mistakes.",
        name='multiply',
        tool_call_id='tool_call_id',
        status='error'
    )
]}
```

:::

:::js
LangGraph provides built-in error handling for tool execution through the prebuilt [ToolNode](https://js.langchain.com/docs/api/langgraph_prebuilt/classes/ToolNode.html) component, used both independently and in prebuilt agents.

By **default**, `ToolNode` catches exceptions raised during tool execution and returns them as `ToolMessage` objects with a status indicating an error.

```typescript
import { AIMessage } from "@langchain/core/messages";
import { ToolNode } from "@langchain/langgraph/prebuilt";
import { tool } from "@langchain/core/tools";
import { z } from "zod";

const multiply = tool(
  (input) => {
    if (input.a === 42) {
      throw new Error("The ultimate error");
    }
    return input.a * input.b;
  },
  {
    name: "multiply",
    description: "Multiply two numbers",
    schema: z.object({
      a: z.number(),
      b: z.number(),
    }),
  }
);

// Default error handling (enabled by default)
const toolNode = new ToolNode([multiply]);

const message = new AIMessage({
  content: "",
  tool_calls: [
    {
      name: "multiply",
      args: { a: 42, b: 7 },
      id: "tool_call_id",
      type: "tool_call",
    },
  ],
});

const result = await toolNode.invoke({ messages: [message] });
```

Output:

```
{ messages: [
  ToolMessage {
    content: "Error: The ultimate error\n Please fix your mistakes.",
    name: "multiply",
    tool_call_id: "tool_call_id",
    status: "error"
  }
]}
```

:::

#### Disable error handling

To propagate exceptions directly, disable error handling:

:::python

```python
tool_node = ToolNode([multiply], handle_tool_errors=False)
```

:::

:::js

```typescript
const toolNode = new ToolNode([multiply], { handleToolErrors: false });
```

:::

With error handling disabled, exceptions raised by tools will propagate up, requiring explicit management.

#### Custom error messages

Provide a custom error message by setting the error handling parameter to a string:

:::python

```python
tool_node = ToolNode(
    [multiply],
    handle_tool_errors="Can't use 42 as the first operand, please switch operands!"
)
```

Example output:

```python
{'messages': [
    ToolMessage(
        content="Can't use 42 as the first operand, please switch operands!",
        name='multiply',
        tool_call_id='tool_call_id',
        status='error'
    )
]}
```

:::

:::js

```typescript
const toolNode = new ToolNode([multiply], {
  handleToolErrors:
    "Can't use 42 as the first operand, please switch operands!",
});
```

Example output:

```typescript
{ messages: [
  ToolMessage {
    content: "Can't use 42 as the first operand, please switch operands!",
    name: "multiply",
    tool_call_id: "tool_call_id",
    status: "error"
  }
]}
```

:::

#### Error handling in agents

:::python
Error handling in prebuilt agents (`create_react_agent`) leverages `ToolNode`:

```python
from langgraph.prebuilt import create_react_agent

agent = create_react_agent(
    model="anthropic:claude-3-7-sonnet-latest",
    tools=[multiply]
)

# Default error handling
agent.invoke({"messages": [{"role": "user", "content": "what's 42 x 7?"}]})
```

To disable or customize error handling in prebuilt agents, explicitly pass a configured `ToolNode`:

```python
custom_tool_node = ToolNode(
    [multiply],
    handle_tool_errors="Cannot use 42 as a first operand!"
)

agent_custom = create_react_agent(
    model="anthropic:claude-3-7-sonnet-latest",
    tools=custom_tool_node
)

agent_custom.invoke({"messages": [{"role": "user", "content": "what's 42 x 7?"}]})
```

:::

:::js
Error handling in prebuilt agents (`createReactAgent`) leverages `ToolNode`:

```typescript
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { ChatAnthropic } from "@langchain/anthropic";

const agent = createReactAgent({
  llm: new ChatAnthropic({ model: "claude-3-5-sonnet-20240620" }),
  tools: [multiply],
});

// Default error handling
await agent.invoke({
  messages: [{ role: "user", content: "what's 42 x 7?" }],
});
```

To disable or customize error handling in prebuilt agents, explicitly pass a configured `ToolNode`:

```typescript
const customToolNode = new ToolNode([multiply], {
  handleToolErrors: "Cannot use 42 as a first operand!",
});

const agentCustom = createReactAgent({
  llm: new ChatAnthropic({ model: "claude-3-5-sonnet-20240620" }),
  tools: customToolNode,
});

await agentCustom.invoke({
  messages: [{ role: "user", content: "what's 42 x 7?" }],
});
```

:::

### Handle large numbers of tools

As the number of available tools grows, you may want to limit the scope of the LLM's selection, to decrease token consumption and to help manage sources of error in LLM reasoning.

To address this, you can dynamically adjust the tools available to a model by retrieving relevant tools at runtime using semantic search.

See [`langgraph-bigtool`](https://github.com/langchain-ai/langgraph-bigtool) prebuilt library for a ready-to-use implementation.

## Prebuilt tools

### LLM provider tools

:::python
You can use prebuilt tools from model providers by passing a dictionary with tool specs to the `tools` parameter of `create_react_agent`. For example, to use the `web_search_preview` tool from OpenAI:

```python
from langgraph.prebuilt import create_react_agent

agent = create_react_agent(
    model="openai:gpt-4o-mini",
    tools=[{"type": "web_search_preview"}]
)
response = agent.invoke(
    {"messages": ["What was a positive news story from today?"]}
)
```

Please consult the documentation for the specific model you are using to see which tools are available and how to use them.
:::

:::js
You can use prebuilt tools from model providers by passing a dictionary with tool specs to the `tools` parameter of `createReactAgent`. For example, to use the `web_search_preview` tool from OpenAI:

```typescript
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { ChatOpenAI } from "@langchain/openai";

const agent = createReactAgent({
  llm: new ChatOpenAI({ model: "gpt-4o-mini" }),
  tools: [{ type: "web_search_preview" }],
});

const response = await agent.invoke({
  messages: [
    { role: "user", content: "What was a positive news story from today?" },
  ],
});
```

Please consult the documentation for the specific model you are using to see which tools are available and how to use them.
:::

### LangChain tools

Additionally, LangChain supports a wide range of prebuilt tool integrations for interacting with APIs, databases, file systems, web data, and more. These tools extend the functionality of agents and enable rapid development.

:::python
You can browse the full list of available integrations in the [LangChain integrations directory](https://python.langchain.com/docs/integrations/tools/).

Some commonly used tool categories include:

- **Search**: Bing, SerpAPI, Tavily
- **Code interpreters**: Python REPL, Node.js REPL
- **Databases**: SQL, MongoDB, Redis
- **Web data**: Web scraping and browsing
- **APIs**: OpenWeatherMap, NewsAPI, and others

These integrations can be configured and added to your agents using the same `tools` parameter shown in the examples above.
:::

:::js
You can browse the full list of available integrations in the [LangChain integrations directory](https://js.langchain.com/docs/integrations/tools/).

Some commonly used tool categories include:

- **Search**: Tavily, SerpAPI
- **Code interpreters**: Web browsers, calculators
- **Databases**: SQL, vector databases
- **Web data**: Web scraping and browsing
- **APIs**: Various API integrations

These integrations can be configured and added to your agents using the same `tools` parameter shown in the examples above.
:::

```
---
https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/use-functional-api.md
```md
# Use the functional API

The [**Functional API**](../concepts/functional_api.md) allows you to add LangGraph's key features  [persistence](../concepts/persistence.md), [memory](../how-tos/memory/add-memory.md), [human-in-the-loop](../concepts/human_in_the_loop.md), and [streaming](../concepts/streaming.md)  to your applications with minimal changes to your existing code.

!!! tip

    For conceptual information on the functional API, see [Functional API](../concepts/functional_api.md).

## Creating a simple workflow

When defining an `entrypoint`, input is restricted to the first argument of the function. To pass multiple inputs, you can use a dictionary.

:::python

```python
@entrypoint(checkpointer=checkpointer)
def my_workflow(inputs: dict) -> int:
    value = inputs["value"]
    another_value = inputs["another_value"]
    ...

my_workflow.invoke({"value": 1, "another_value": 2})
```

:::

:::js

```typescript
const checkpointer = new MemorySaver();

const myWorkflow = entrypoint(
  { checkpointer, name: "myWorkflow" },
  async (inputs: { value: number; anotherValue: number }) => {
    const value = inputs.value;
    const anotherValue = inputs.anotherValue;
    // ...
  }
);

await myWorkflow.invoke({ value: 1, anotherValue: 2 });
```

:::

??? example "Extended example: simple workflow"

    :::python
    ```python
    import uuid
    from langgraph.func import entrypoint, task
    from langgraph.checkpoint.memory import InMemorySaver

    # Task that checks if a number is even
    @task
    def is_even(number: int) -> bool:
        return number % 2 == 0

    # Task that formats a message
    @task
    def format_message(is_even: bool) -> str:
        return "The number is even." if is_even else "The number is odd."

    # Create a checkpointer for persistence
    checkpointer = InMemorySaver()

    @entrypoint(checkpointer=checkpointer)
    def workflow(inputs: dict) -> str:
        """Simple workflow to classify a number."""
        even = is_even(inputs["number"]).result()
        return format_message(even).result()

    # Run the workflow with a unique thread ID
    config = {"configurable": {"thread_id": str(uuid.uuid4())}}
    result = workflow.invoke({"number": 7}, config=config)
    print(result)
    ```
    :::

    :::js
    ```typescript
    import { v4 as uuidv4 } from "uuid";
    import { entrypoint, task, MemorySaver } from "@langchain/langgraph";

    // Task that checks if a number is even
    const isEven = task("isEven", async (number: number) => {
      return number % 2 === 0;
    });

    // Task that formats a message
    const formatMessage = task("formatMessage", async (isEven: boolean) => {
      return isEven ? "The number is even." : "The number is odd.";
    });

    // Create a checkpointer for persistence
    const checkpointer = new MemorySaver();

    const workflow = entrypoint(
      { checkpointer, name: "workflow" },
      async (inputs: { number: number }) => {
        // Simple workflow to classify a number
        const even = await isEven(inputs.number);
        return await formatMessage(even);
      }
    );

    // Run the workflow with a unique thread ID
    const config = { configurable: { thread_id: uuidv4() } };
    const result = await workflow.invoke({ number: 7 }, config);
    console.log(result);
    ```
    :::

??? example "Extended example: Compose an essay with an LLM"

    This example demonstrates how to use the `@task` and `@entrypoint` decorators
    syntactically. Given that a checkpointer is provided, the workflow results will
    be persisted in the checkpointer.

    :::python
    ```python
    import uuid
    from langchain.chat_models import init_chat_model
    from langgraph.func import entrypoint, task
    from langgraph.checkpoint.memory import InMemorySaver

    llm = init_chat_model('openai:gpt-3.5-turbo')

    # Task: generate essay using an LLM
    @task
    def compose_essay(topic: str) -> str:
        """Generate an essay about the given topic."""
        return llm.invoke([
            {"role": "system", "content": "You are a helpful assistant that writes essays."},
            {"role": "user", "content": f"Write an essay about {topic}."}
        ]).content

    # Create a checkpointer for persistence
    checkpointer = InMemorySaver()

    @entrypoint(checkpointer=checkpointer)
    def workflow(topic: str) -> str:
        """Simple workflow that generates an essay with an LLM."""
        return compose_essay(topic).result()

    # Execute the workflow
    config = {"configurable": {"thread_id": str(uuid.uuid4())}}
    result = workflow.invoke("the history of flight", config=config)
    print(result)
    ```
    :::

    :::js
    ```typescript
    import { v4 as uuidv4 } from "uuid";
    import { ChatOpenAI } from "@langchain/openai";
    import { entrypoint, task, MemorySaver } from "@langchain/langgraph";

    const llm = new ChatOpenAI({ model: "gpt-3.5-turbo" });

    // Task: generate essay using an LLM
    const composeEssay = task("composeEssay", async (topic: string) => {
      // Generate an essay about the given topic
      const response = await llm.invoke([
        { role: "system", content: "You are a helpful assistant that writes essays." },
        { role: "user", content: `Write an essay about ${topic}.` }
      ]);
      return response.content as string;
    });

    // Create a checkpointer for persistence
    const checkpointer = new MemorySaver();

    const workflow = entrypoint(
      { checkpointer, name: "workflow" },
      async (topic: string) => {
        // Simple workflow that generates an essay with an LLM
        return await composeEssay(topic);
      }
    );

    // Execute the workflow
    const config = { configurable: { thread_id: uuidv4() } };
    const result = await workflow.invoke("the history of flight", config);
    console.log(result);
    ```
    :::

## Parallel execution

Tasks can be executed in parallel by invoking them concurrently and waiting for the results. This is useful for improving performance in IO bound tasks (e.g., calling APIs for LLMs).

:::python

```python
@task
def add_one(number: int) -> int:
    return number + 1

@entrypoint(checkpointer=checkpointer)
def graph(numbers: list[int]) -> list[str]:
    futures = [add_one(i) for i in numbers]
    return [f.result() for f in futures]
```

:::

:::js

```typescript
const addOne = task("addOne", async (number: number) => {
  return number + 1;
});

const graph = entrypoint(
  { checkpointer, name: "graph" },
  async (numbers: number[]) => {
    return await Promise.all(numbers.map(addOne));
  }
);
```

:::

??? example "Extended example: parallel LLM calls"

    This example demonstrates how to run multiple LLM calls in parallel using `@task`. Each call generates a paragraph on a different topic, and results are joined into a single text output.

    :::python
    ```python
    import uuid
    from langchain.chat_models import init_chat_model
    from langgraph.func import entrypoint, task
    from langgraph.checkpoint.memory import InMemorySaver

    # Initialize the LLM model
    llm = init_chat_model("openai:gpt-3.5-turbo")

    # Task that generates a paragraph about a given topic
    @task
    def generate_paragraph(topic: str) -> str:
        response = llm.invoke([
            {"role": "system", "content": "You are a helpful assistant that writes educational paragraphs."},
            {"role": "user", "content": f"Write a paragraph about {topic}."}
        ])
        return response.content

    # Create a checkpointer for persistence
    checkpointer = InMemorySaver()

    @entrypoint(checkpointer=checkpointer)
    def workflow(topics: list[str]) -> str:
        """Generates multiple paragraphs in parallel and combines them."""
        futures = [generate_paragraph(topic) for topic in topics]
        paragraphs = [f.result() for f in futures]
        return "\n\n".join(paragraphs)

    # Run the workflow
    config = {"configurable": {"thread_id": str(uuid.uuid4())}}
    result = workflow.invoke(["quantum computing", "climate change", "history of aviation"], config=config)
    print(result)
    ```
    :::

    :::js
    ```typescript
    import { v4 as uuidv4 } from "uuid";
    import { ChatOpenAI } from "@langchain/openai";
    import { entrypoint, task, MemorySaver } from "@langchain/langgraph";

    // Initialize the LLM model
    const llm = new ChatOpenAI({ model: "gpt-3.5-turbo" });

    // Task that generates a paragraph about a given topic
    const generateParagraph = task("generateParagraph", async (topic: string) => {
      const response = await llm.invoke([
        { role: "system", content: "You are a helpful assistant that writes educational paragraphs." },
        { role: "user", content: `Write a paragraph about ${topic}.` }
      ]);
      return response.content as string;
    });

    // Create a checkpointer for persistence
    const checkpointer = new MemorySaver();

    const workflow = entrypoint(
      { checkpointer, name: "workflow" },
      async (topics: string[]) => {
        // Generates multiple paragraphs in parallel and combines them
        const paragraphs = await Promise.all(topics.map(generateParagraph));
        return paragraphs.join("\n\n");
      }
    );

    // Run the workflow
    const config = { configurable: { thread_id: uuidv4() } };
    const result = await workflow.invoke(["quantum computing", "climate change", "history of aviation"], config);
    console.log(result);
    ```
    :::

    This example uses LangGraph's concurrency model to improve execution time, especially when tasks involve I/O like LLM completions.

## Calling graphs

The **Functional API** and the [**Graph API**](../concepts/low_level.md) can be used together in the same application as they share the same underlying runtime.

:::python

```python
from langgraph.func import entrypoint
from langgraph.graph import StateGraph

builder = StateGraph()
...
some_graph = builder.compile()

@entrypoint()
def some_workflow(some_input: dict) -> int:
    # Call a graph defined using the graph API
    result_1 = some_graph.invoke(...)
    # Call another graph defined using the graph API
    result_2 = another_graph.invoke(...)
    return {
        "result_1": result_1,
        "result_2": result_2
    }
```

:::

:::js

```typescript
import { entrypoint } from "@langchain/langgraph";
import { StateGraph } from "@langchain/langgraph";

const builder = new StateGraph(/* ... */);
// ...
const someGraph = builder.compile();

const someWorkflow = entrypoint(
  { name: "someWorkflow" },
  async (someInput: Record<string, any>) => {
    // Call a graph defined using the graph API
    const result1 = await someGraph.invoke(/* ... */);
    // Call another graph defined using the graph API
    const result2 = await anotherGraph.invoke(/* ... */);
    return {
      result1,
      result2,
    };
  }
);
```

:::

??? example "Extended example: calling a simple graph from the functional API"

    :::python
    ```python
    import uuid
    from typing import TypedDict
    from langgraph.func import entrypoint
    from langgraph.checkpoint.memory import InMemorySaver
    from langgraph.graph import StateGraph

    # Define the shared state type
    class State(TypedDict):
        foo: int

    # Define a simple transformation node
    def double(state: State) -> State:
        return {"foo": state["foo"] * 2}

    # Build the graph using the Graph API
